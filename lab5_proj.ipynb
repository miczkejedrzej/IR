{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system\n",
    "The idea of the project is to predict ratings of unwatched movies  for userId 610 in order to be able to recommend movies to this user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(156068)\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = pd.read_csv('./ratings.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the dataframe to the convinient form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows Ã— 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "movieId                                                    ...                  \n",
       "1        4.0  NaN  NaN  NaN  4.0  NaN  4.5  NaN  NaN  NaN  ...  4.0  NaN  4.0   \n",
       "2        NaN  NaN  NaN  NaN  NaN  4.0  NaN  4.0  NaN  NaN  ...  NaN  4.0  NaN   \n",
       "3        4.0  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "4        NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "5        NaN  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "\n",
       "userId   604  605  606  607  608  609  610  \n",
       "movieId                                     \n",
       "1        3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
       "2        5.0  3.5  NaN  NaN  2.0  NaN  NaN  \n",
       "3        NaN  NaN  NaN  NaN  2.0  NaN  NaN  \n",
       "4        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "5        3.0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userId = 610\n",
    "transformed_df = df.pivot(index='movieId', columns='userId', values='rating')\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coorelation finding methods\n",
    "Here there are the functions used for finding the coorelations between the users basing on the mutual movies that  they have watched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the coorelation finding method which just finds the pearson coorelation between the  given user and all other users , \n",
    "if they have at least 2 movies in common\n",
    "and if neither given user nor other user ratings for common films are constant array '''\n",
    "def find_correlation(transformed_df:pd.DataFrame,userId=610):\n",
    "    user_column = transformed_df[userId].copy()\n",
    "    user_column.dropna(inplace=True)\n",
    "    correlations = dict()\n",
    "    \n",
    "\n",
    "\n",
    "    for other_user in transformed_df.drop(userId,axis=1).columns:\n",
    "        #cannot check correlations between arrays with 1 element \n",
    "        common_ratings = user_column.index.intersection(transformed_df[other_user].dropna().index)\n",
    "        if len(common_ratings)>1: \n",
    "            common_ratings = list(common_ratings) \n",
    "            \n",
    "            # cannot check correlation with array of all elements with the same value, since the concept of  correlation does not apply there\n",
    "            if user_column[common_ratings].nunique() > 1 and transformed_df[other_user][common_ratings].nunique() > 1:\n",
    "                corr, _ = pearsonr(user_column[common_ratings], transformed_df[other_user][common_ratings])\n",
    "                correlations[other_user] = corr\n",
    "                continue \n",
    "\n",
    "        correlations[other_user] = np.nan  \n",
    "\n",
    "\n",
    "    correlation_df = pd.Series(correlations)\n",
    "\n",
    "    return correlation_df\n",
    "\n",
    "\n",
    "'''the coorelation finding function as above,\n",
    " however  the coorelation is found only if the user and other user have at least threshold films in common'''\n",
    "def find_correlation_with_common_films_threshold(transformed_df:pd.DataFrame,threshold=5,userId=610):\n",
    "    user_column = transformed_df[userId].copy()\n",
    "    user_column.dropna(inplace=True)\n",
    "    correlations = dict()\n",
    "    \n",
    "\n",
    "\n",
    "    for other_user in transformed_df.drop(userId,axis=1).columns:\n",
    "        #cannot check correlations between arrays with 1 element \n",
    "        common_ratings = user_column.index.intersection(transformed_df[other_user].dropna().index)\n",
    "        if len(common_ratings)>threshold: \n",
    "            common_ratings = list(common_ratings) \n",
    "            # cannot check correlation with array of all elements with the same value, since the concept of  correlation does not apply there\n",
    "            if user_column[common_ratings].nunique() > 1 and transformed_df[other_user][common_ratings].nunique() > 1:\n",
    "                corr, _ = pearsonr(user_column[common_ratings], transformed_df[other_user][common_ratings])\n",
    "                correlations[other_user] = corr\n",
    "                continue \n",
    "\n",
    "        correlations[other_user] = np.nan  \n",
    "\n",
    "    correlation_df = pd.Series(correlations)\n",
    "    \n",
    "\n",
    "\n",
    "    return correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie recommender systems\n",
    "Here can be found the different movie recommender systems approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_scores(transformed_df: pd.DataFrame, userId=610, k=10,split = None):\n",
    "    '''we find up to k most coorelated users with our userID  that have rated given movie not seen by our userID\n",
    "        and calculate rating  as a weighted average\n",
    "        of their ratings for this film. up to k because it may happen that there are less than k users \n",
    "        with positive correlation with our userId who watched\n",
    "        given movie'''\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    sorted_correlations = sorted_correlations[sorted_correlations > 0]\n",
    "    sorted_transposed_df = transposed_df.loc[sorted_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "\n",
    " \n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "       \n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            top_users_indices = sorted_correlations.index.intersection(valid_ratings.index)[:k]\n",
    "            if top_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = valid_ratings[top_users_indices].dot(user_correlations[top_users_indices]) / user_correlations[top_users_indices].sum()\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "\n",
    "    \n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_scores_no_baseline(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    \n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using top `k` most correlated users. Rating for tuple (userId,movie) predicted as mean of ratings for our userID for all movies + \n",
    "    weighted deviation from the mean for given movie of the k most correlated users.\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    user_means = transposed_df.mean(axis=1, skipna=True)\n",
    "    user_mean_target = user_means[userId]\n",
    "    predictions_df = transposed_df.copy()\n",
    "\n",
    "    for movie in transposed_df.columns:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "        ratings_for_movie = transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_sorted = sorted_correlations.index.intersection(valid_ratings.index)\n",
    "            top_k_users = valid_users_sorted[:k]\n",
    "            if len(top_k_users) == 0:\n",
    "                continue\n",
    "            weighted_deviation_sum = 0\n",
    "            total_weight = 0\n",
    "\n",
    "            for user in top_k_users:\n",
    "                correlation = sorted_correlations[user]  \n",
    "                user_mean = user_means[user]  \n",
    "                user_rating = ratings_for_movie[user] \n",
    "                deviation = user_rating - user_mean\n",
    "                weighted_deviation_sum += correlation * deviation\n",
    "                total_weight += abs(correlation)\n",
    "            if total_weight > 0:\n",
    "                weighted_deviation = weighted_deviation_sum / total_weight\n",
    "                predicted_score = user_mean_target + weighted_deviation\n",
    "                predictions_df.loc[userId, movie] = predicted_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "def predict_scores_with_item_correction(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    border  =0.8,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    works similar to the predictor above, only at the end it adjusts the  rating basing on the mean rating for given film in case\n",
    "    the rating is close to the border of ratings \n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    user_means = transposed_df.mean(axis=1, skipna=True)\n",
    "    item_means = transformed_df.mean(axis=1, skipna=True)\n",
    "    user_mean_target = user_means[userId]\n",
    "    predictions_df = transposed_df.copy()\n",
    "\n",
    "    for movie in transposed_df.columns:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "        ratings_for_movie = transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_sorted = sorted_correlations.index.intersection(valid_ratings.index)\n",
    "            top_k_users = valid_users_sorted[:k]\n",
    "            if len(top_k_users) == 0:\n",
    "                continue\n",
    "            weighted_deviation_sum = 0\n",
    "            total_weight = 0\n",
    "\n",
    "            for user in top_k_users:\n",
    "                correlation = sorted_correlations[user]  \n",
    "                user_mean = user_means[user]  \n",
    "                user_rating = ratings_for_movie[user]  \n",
    "                deviation = user_rating - user_mean\n",
    "                weighted_deviation_sum += correlation * deviation\n",
    "                total_weight += abs(correlation)\n",
    "            \n",
    "            weighted_deviation = weighted_deviation_sum / total_weight if total_weight > 0 else 0\n",
    "            item_mean = item_means[movie] if movie in item_means else 0\n",
    "            if item_mean >= round(user_mean_target*2)/2  and 2*user_mean_target- int(2*user_mean_target) >= border:\n",
    "                correction = 0.5\n",
    "            elif item_mean <= round(user_mean_target*2)/2  and 2*user_mean_target- int(2*user_mean_target) <= (1-border):\n",
    "                correction = - 0.5\n",
    "            else:\n",
    "                correction = 0\n",
    "\n",
    "            predicted_score = user_mean_target + weighted_deviation + correction\n",
    "            predicted_score = round(predicted_score * 2) / 2  \n",
    "            predicted_score = max(0.5,predicted_score)\n",
    "            predictions_df.loc[userId, movie] = predicted_score\n",
    "\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_baseline(transformed_df: pd.DataFrame, userId=610, k=1,split=None):\n",
    "    \"\"\"\n",
    "    Predict baseline scores for movies the user has not rated.\n",
    "    mean of user ratings of this user + movie ratings for this movie\n",
    "    \"\"\"\n",
    "    predictions_df = transformed_df.copy()\n",
    "    for movie in transformed_df.index:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "\n",
    "        if pd.isna(transformed_df.loc[movie, userId]):\n",
    "            user_ratings = transformed_df[userId].dropna()\n",
    "            movie_ratings = transformed_df.loc[movie].dropna()\n",
    "            total_ratings_sum = user_ratings.sum() + movie_ratings.sum()\n",
    "            total_ratings_count = len(user_ratings) + len(movie_ratings)\n",
    "            if total_ratings_count > 0:\n",
    "                baseline_prediction = total_ratings_sum / total_ratings_count\n",
    "                predictions_df.loc[movie, userId] = round(baseline_prediction * 2) / 2  # Round to nearest 0.5\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_scores_with_threshold(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=1,\n",
    "    threshold=0.8,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using all correlated users above a specified threshold.\n",
    "    weighted average of these users  rating for this film . Weights are the coorelation\n",
    "    \n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    filtered_correlations = user_correlations[user_correlations > threshold]\n",
    "    sorted_transposed_df = transposed_df.loc[filtered_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_indices =filtered_correlations.index.intersection(valid_ratings.index)\n",
    "            if valid_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = (\n",
    "                valid_ratings[valid_users_indices].dot(filtered_correlations[valid_users_indices]) /\n",
    "                filtered_correlations[valid_users_indices].sum()\n",
    "            )\n",
    "\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "def predict_scores_with_threshold_negative(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=1,\n",
    "    positive_threshold=0,\n",
    "    negative_threshold=0,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using all correlated users above a specified threshold for positive correlations,\n",
    "    and include negatively correlated users below a specified threshold with adjusted contribution.\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    positive_correlations = user_correlations[user_correlations > positive_threshold]\n",
    "    negative_correlations = user_correlations[\n",
    "        (user_correlations < 0) & (user_correlations.abs() > negative_threshold)\n",
    "    ]\n",
    "    sorted_transposed_df = transposed_df.loc[\n",
    "        positive_correlations.index.union(negative_correlations.index)\n",
    "    ]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            positive_users = positive_correlations.index.intersection(valid_ratings.index)\n",
    "            negative_users = negative_correlations.index.intersection(valid_ratings.index)\n",
    "            weighted_sum = 0\n",
    "            total_weight = 0\n",
    "            if not positive_users.empty:\n",
    "                weighted_sum += (\n",
    "                    valid_ratings[positive_users].dot(positive_correlations[positive_users])\n",
    "                )\n",
    "                total_weight += positive_correlations[positive_users].sum()\n",
    "\n",
    "            for neg_user in negative_users:\n",
    "                neg_rating = valid_ratings[neg_user]\n",
    "                if neg_rating < 2 or neg_rating > 4: \n",
    "                    adjusted_rating = 6 - neg_rating\n",
    "                    weight = abs(negative_correlations[neg_user])\n",
    "                    weighted_sum += adjusted_rating * weight\n",
    "                    total_weight += weight\n",
    "\n",
    "            if total_weight == 0:\n",
    "                continue\n",
    "\n",
    "            weighted_avg = weighted_sum / total_weight\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_scores_strict_correlation(transformed_df: pd.DataFrame, userId=610, k=10,correlation_neighbours=5,split = None):\n",
    "\n",
    "    '''function working the same as predict_scores described above, with only difference , that coorelation are defined  only for the users\n",
    "    having at least coorelation_neighbours mutual watched films with userId '''\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation_with_common_films_threshold(transformed_df,correlation_neighbours,userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    sorted_correlations = sorted_correlations[sorted_correlations > 0]\n",
    "    sorted_transposed_df = transposed_df.loc[sorted_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            top_users_indices = sorted_correlations.index.intersection(valid_ratings.index)[:k]\n",
    "            if top_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = valid_ratings[top_users_indices].dot(user_correlations[top_users_indices]) / user_correlations[top_users_indices].sum()\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance of predicton methods\n",
    "\n",
    "Frameworks for evaluating the recommender systems. All of them are based on the 10-fold cross validation on the userID 610, ensuring the splits are always the same across every run of evaluation methods , so  that the comparison of different recommender systems is plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendation_quality_general(\n",
    "    transformed_df,\n",
    "    predict_function,  \n",
    "    userId=610,\n",
    "    k=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs  \n",
    "):\n",
    "    \"\"\"\n",
    "    General evaluation function for recommendation quality using Mae. \n",
    "    \"\"\"\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index  \n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)  \n",
    "        mae = np.abs(predicted_scores[valid_indices] - actual_scores[valid_indices]).mean() \n",
    "        \n",
    "        mae_list.append(mae)\n",
    "      \n",
    "    \n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def evaluate_recommendation_quality_at_p(\n",
    "    transformed_df,\n",
    "    predict_function,  \n",
    "    userId=610,\n",
    "    k=10,\n",
    "    p=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs  \n",
    "):\n",
    "    \"\"\"\n",
    "    Function for evaluating the Mae at p  highest rated items.\n",
    "    \"\"\"\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index  \n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        predicted_scores.sort_values(ascending=False)\n",
    "        predicted_scores = predicted_scores[:p]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)  \n",
    "        mae = np.abs(predicted_scores[valid_indices] - actual_scores[valid_indices]).mean()  \n",
    "        mae_list.append(mae)\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def evaluate_recommendation_quality_rmse_general(\n",
    "    transformed_df,\n",
    "    predict_function,  \n",
    "    userId=610,\n",
    "    k=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs  \n",
    "):\n",
    "    \"\"\"\n",
    "    General evaluation function for recommendation quality using RMSE.\n",
    "    \"\"\"\n",
    "    \n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index  \n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    rmse_list = []\n",
    "\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores) \n",
    "        rmse = np.sqrt(((predicted_scores[valid_indices] - actual_scores[valid_indices]) ** 2).mean())  \n",
    "        rmse_list.append(rmse)\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_recommendation_quality_general_weighted(\n",
    "transformed_df,predict_function,\n",
    " weight_under=1,weight_over=1,\n",
    " userId=610, k=20, num_splits=10,\n",
    "**predict_function_kwargs):\n",
    "    '''function used to evalue the overestimation error and underestimation error'''\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index \n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan,userId=userId, k=k, split=split,**predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)  \n",
    "        error_array = predicted_scores[valid_indices] - actual_scores[valid_indices]\n",
    "        underpredicted_error = error_array[error_array<=0]\n",
    "        overpredicted_error = error_array[error_array>0]\n",
    "        underpredicted_error = np.abs(underpredicted_error)\n",
    "        underpredicted_error  = underpredicted_error * weight_under\n",
    "        overpredicted_error = overpredicted_error * weight_over\n",
    "        weighted_mae = (sum(underpredicted_error) + sum(overpredicted_error)) / (len(underpredicted_error)*weight_under + len(overpredicted_error)*weight_over)\n",
    "        mae_list.append(weighted_mae)\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of testing \n",
    "\n",
    "We decided to test several movie recommender systems, which will be duly described in each step, as well as  the conclusions of the results  of the  given  systems. In order to test the predicition accuracy of the system we have devised several different evaluation methods. All of them are basing on the 10-fold  cross validation on the userId 610. It means we run 10 iterations of the recommender systems, in each giving it 9/10 of the records for userID 610 and all the records for other users and we command it to predict score for the deliberately erased data for userId 610 . We calculate the error and take the mean of the error for the 10 runs. Surely we would get more data if we tried to do the same approach with several other users, but since our task is specifically to predict the ratings for the movies for userId 610 , we base the evaluation only on the algorithms error on this userID. When  creating these splits for 10-fold validation we do not shuffle the data as it is not anyhow sorted byt rating for the Userid 610 and we would want to always evaluate the recommender systems on the exactly same splits to compare them as good as possible. As we have  mentioned there are several methods of error evaluations, mainly:\n",
    "- MAE , simply the mean absolute error , very easy to interpret measure\n",
    "- MRSE, the mean root squared error, a good measure for this specific purpose, as it penalizes biggger mistakes more\n",
    "- Weighted MAE , the idea is that  our user surely will have ceratain preference. The overestimating may be more detrimental to the user (if he is very picky or  doesnt have much time for watching the movies) then what matters more is the precision of high ratings. On the other hand if the user spends all his day watching films, he probably would not really care  much for the false positive, but on the other hand, he would not want to run out of movie\n",
    "choices, therefore false negative high rating is bad for him. So to summ up , we do the mae but we set weight to the overestimation error and weight to \n",
    "underestimation erorr, and we can alter these weights to see best  systems for high precision in high ratings and best  systems for high recall for high ratings. To have only one type of the error accounted for we will use weights (1,0) and (0,1), so  accounting only for one type of error.\n",
    "- Lastly we have a mea but only for the k  best rated films . Since surely user may not care that much about the ratings of  the films he wouldnt watch anyway , it is maybe  somehow similar to the previous approach overestimation error, since it would work good for a picky user with small amount of time - he doesnt want to watch some crap, but wont care that much about ratings of films he wont have time to watch anyway.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remark\n",
    "- What we have not mentioned explicitly earlier , each system when having final prediction will round it to the closest viable value, because we want to obtain correct prediction in  the given scale i.e  for the ratings.csv from 0.5 to 5 with change by 0.5\n",
    "- when referring to the correlation between users, we always refer to the pearson coorelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we will store best errors, to be able later to seem which system performed best for which purpose\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "perror_list = []\n",
    "overestimate_error_list = []\n",
    "underestimate_error_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline predictor\n",
    "Firstly we would like to check the performance of the baseline predictor, to have something to compare with later. The baseline predictor is a very simple approach which  when predicting score for given tuple (movie,userID) returns  mean of all the ratings for given movie + all the ratings for given userID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using the baseline predictio 0.6701644157369347\n",
      "RMSE using the baseline predictio 0.8670309338053681\n",
      "Mae at p=10 using the baseline prediction 0.675\n",
      "Overestimate MAE is 0.8334233572346182 \n",
      "Underestimate  MAE is 0.5890073252784332 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mae = evaluate_recommendation_quality_general(\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_baseline,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10 # Pass the threshold as a keyword argument\n",
    "    )\n",
    "mae_list.append(('predict_baseline',mae))\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_baseline,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10 # Pass the threshold as a keyword argument\n",
    "    )\n",
    "print(f\"MAE using the baseline predictio {mae}\")\n",
    "print(f\"RMSE using the baseline predictio {rmse}\")\n",
    "rmse_list.append(('predict_baseline',rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "        p=10,\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_baseline,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10 # Pass the threshold as a keyword argument\n",
    "    )\n",
    "print(f\"Mae at p=10 using the baseline prediction {p_error}\")\n",
    "perror_list.append(('predict_baseline',p_error))\n",
    "\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=0,\n",
    "        weight_over=1,\n",
    "        predict_function=predict_baseline,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10 # Pass the threshold as a keyword argument\n",
    "    )\n",
    "print(f\"Overestimate MAE is {overestimate_error} \")\n",
    "overestimate_error_list.append(('predict_baseline',overestimate_error))\n",
    "\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=1,\n",
    "        weight_over=0,\n",
    "        userId=610,\n",
    "        predict_function=predict_baseline,  # Function to predict scores\n",
    "        num_splits=10 # Pass the threshold as a keyword argument\n",
    "    )\n",
    "print(f\"Underestimate  MAE is {underestimate_error} \")\n",
    "underestimate_error_list.append(('predict_baseline',underestimate_error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusions\n",
    "Honestly it does not  perform that terribly in terms of the mae , it can be a good computationaly cheap start. One thing is that this  system, at least for this data, turned out quite biased, overestimating error is significantly higher than the underestimating error, so it tends to overestimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user coorelation approaches\n",
    "now , as we have something to compare our more sophisticated  recommender systems with , we can start to test them. The first system  sets the prediction for tuple (userID,movie) as weighted average of ratings for  given movie for users which watched  given movie and had the positive correlation with the given user with value of the coorelation above some threshold. we will check several corrrelation thresholds above which we include the users to the weighted mean and see which one performs the best . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 0 positive coorelation threshold 0.6021952781071522\n",
      "RMSE using 0 positive coorelation threshold 0.7939195306055609\n",
      "Mae at p=10 using 0% positive coorelation threshold 0.505952380952381\n",
      "Overestimate MAE is 0.6211578619909098 \n",
      "underestimate_error using 0% positive coorelation threshold 0.574598358638441\n",
      "MAE using 0.2 positive coorelation threshold 0.606241746389952\n",
      "RMSE using 0.2 positive coorelation threshold 0.802867162923574\n",
      "Mae at p=10 using 0.2% positive coorelation threshold 0.5147222222222223\n",
      "Overestimate MAE is 0.619356961510738 \n",
      "underestimate_error using 0.2% positive coorelation threshold 0.5873767604119139\n",
      "MAE using 0.4 positive coorelation threshold 0.6355510566787939\n",
      "RMSE using 0.4 positive coorelation threshold 0.8408924823724673\n",
      "Mae at p=10 using 0.4% positive coorelation threshold 0.5407936507936507\n",
      "Overestimate MAE is 0.6449831749158503 \n",
      "underestimate_error using 0.4% positive coorelation threshold 0.6224698281566787\n",
      "MAE using 0.6 positive coorelation threshold 0.737954359048584\n",
      "RMSE using 0.6 positive coorelation threshold 0.933130514623205\n",
      "Mae at p=10 using 0.6% positive coorelation threshold nan\n",
      "Overestimate MAE is 0.7608267474862302 \n",
      "underestimate_error using 0.6% positive coorelation threshold 0.6739252769765453\n"
     ]
    }
   ],
   "source": [
    "for correlation_threshold in [0, 0.2, 0.4, 0.6]:\n",
    "        mae = evaluate_recommendation_quality_general(\n",
    "            transformed_df=transformed_df,\n",
    "            num_splits=10,\n",
    "            predict_function=predict_scores_with_threshold,\n",
    "            userId=610,\n",
    "            threshold=correlation_threshold \n",
    "        )\n",
    "        rmse = evaluate_recommendation_quality_rmse_general(\n",
    "                transformed_df=transformed_df,\n",
    "                predict_function=predict_scores_with_threshold,  \n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                threshold=correlation_threshold \n",
    "            )\n",
    "        print(f\"MAE using {correlation_threshold} positive coorelation threshold {mae}\")\n",
    "        mae_list.append((f\"MAE using {correlation_threshold}% positive coorelation threshold\",mae))\n",
    "        \n",
    "        print(f\"RMSE using {correlation_threshold} positive coorelation threshold {rmse}\")\n",
    "        rmse_list.append((f\"Rmse using {correlation_threshold}% positive coorelation threshold\",rmse))\n",
    "\n",
    "        p_error = evaluate_recommendation_quality_at_p(\n",
    "                p=10,\n",
    "                transformed_df=transformed_df,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_threshold,  \n",
    "                userId=610,\n",
    "                threshold=correlation_threshold \n",
    "            )\n",
    "        print(f\"Mae at p=10 using {correlation_threshold}% positive coorelation threshold {p_error}\")\n",
    "        perror_list.append((f\"Mae at p=10 using {correlation_threshold} positive coorelation threshold\",p_error))\n",
    "\n",
    "        overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=2,\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_threshold,  \n",
    "                threshold=correlation_threshold \n",
    "            )\n",
    "        print(f\"Overestimate MAE is {overestimate_error} \")\n",
    "        overestimate_error_list.append((f\"Overestimate Mae using  {correlation_threshold}% positive coorelation threshold\",overestimate_error))\n",
    "\n",
    "        underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=0,\n",
    "                predict_function=predict_scores_with_threshold,  \n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                threshold=correlation_threshold \n",
    "            )\n",
    "        print(f\"underestimate_error using {correlation_threshold}% positive coorelation threshold {underestimate_error}\")\n",
    "        underestimate_error_list.append((f\"p_error using {correlation_threshold}% positive coorelation threshold\",underestimate_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusions\n",
    "It has some advantages compared  to the baseline approach , however the difference considering mae is not staggering, about ~10% . The Rmse is  decreasing even by lower percentage. However  the predictions are definitely more balanced, i.e the difference between the underestimation_error and over_estimation_error is neglibile in comparison to what it was previously.  Moreover the error at p=10 best ratings decreases visibly, what might be important  in reall recommender systems since, these are the  movies the user would watch if he was to choose by predicted rating. \n",
    "Interestingly, in this case in nearly all metrics,  setting threshold to 0 therefore taking all positively correlated users proved to give the best effects. On the other hand creating the score only basing on users with positive coorelation > 0.6 could even be worse than the basline approach as the Rmse is bigger here! It could be probably casued by the scarcity of data, as it could occur that these users with high coorelation had for example only 3 films in common with our userId 610 , therefore were not very 'reliable' source of information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the negatively coorelated\n",
    "since we do not suffer from abundance of data, it would be great to take adventage of the users which are negatively coorelated to the current user. There is plenty of such  users and we devised an algorithm which can do that. Mainly we will use the positive coorelation as in the previous function, but then we will also check for the negatively coorelated users with the magnitude of the coorelation above some threshold. i.e we want to have strongly negatively coorelated users. Then we  incorporate them in the weighted average , but in doing so , we do not take their ratings as they are, but we change them to rating = 6-rating. Why? because since the relation is negative, if these users rated  given film high, we expect our user to rate it low , so in other words we create sort of a complement of a rating. Also we do that only  for ratings of strongly negatively correalated users in range (0,2> and <4,5>) since for example  if we have strongly negatively correlated user who give 3 for some movie , we would have to give our use also 3 ->6-3 = 3 , but it seems it shall not work this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 0% positive and  0.6%  negative  coorelation threshold 0.6113749808614168\n",
      "RMSE using 0% positive and  0.6%  negative  coorelation threshold 0.80136603031125\n",
      "Ma at p=10 using 0% positive and  0.6%  negative  coorelation threshold0.510952380952381\n",
      "overestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.745418158624428 \n",
      "underestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.5859043019335467\n",
      "MAE using 0% positive and  0.6%  negative  coorelation threshold 0.6060534511840754\n",
      "RMSE using 0% positive and  0.6%  negative  coorelation threshold 0.7972685194076556\n",
      "Ma at p=10 using 0% positive and  0.6%  negative  coorelation threshold0.510952380952381\n",
      "overestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.7347326854718496 \n",
      "underestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.5793087606016811\n",
      "MAE using 0% positive and  0.6%  negative  coorelation threshold 0.604099744608393\n",
      "RMSE using 0% positive and  0.6%  negative  coorelation threshold 0.7953086282698761\n",
      "Ma at p=10 using 0% positive and  0.6%  negative  coorelation threshold0.505952380952381\n",
      "overestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.7360172704520868 \n",
      "underestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.5766572684607717\n"
     ]
    }
   ],
   "source": [
    "for negative_correlation in [0.2,0.4,0.6]:\n",
    "        mae = evaluate_recommendation_quality_general(\n",
    "            transformed_df=transformed_df,\n",
    "            num_splits=10,\n",
    "            predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "            userId=610,\n",
    "            positive_threshold = 0 ,\n",
    "            negative_threshold = negative_correlation\n",
    "             # Pass the threshold as a keyword argument\n",
    "        )\n",
    "        rmse = evaluate_recommendation_quality_rmse_general(\n",
    "                transformed_df=transformed_df,\n",
    "                predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                positive_threshold = 0 ,\n",
    "                negative_threshold = negative_correlation\n",
    "            )\n",
    "        print(f\"MAE using 0% positive and  {correlation_threshold}%  negative  coorelation threshold {mae}\")\n",
    "        mae_list.append((f\"MAE using 0% positive and  {correlation_threshold}%  negative  coorelation threshold\",mae))\n",
    "        \n",
    "        print(f\"RMSE using 0% positive and  {correlation_threshold}%  negative  coorelation threshold {rmse}\")\n",
    "        rmse_list.append((f\"Rmse using {correlation_threshold} positive coorelation threshold\",rmse))\n",
    "\n",
    "        p_error = evaluate_recommendation_quality_at_p(\n",
    "                p=10,\n",
    "                transformed_df=transformed_df,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "                userId=610,\n",
    "                positive_threshold = 0 ,\n",
    "                negative_threshold = negative_correlation\n",
    "            )\n",
    "        print(f\"Ma at p=10 using 0% positive and  {correlation_threshold}%  negative  coorelation threshold{p_error}\")\n",
    "        perror_list.append((f\"p_error using 0% positive and  {correlation_threshold}%  negative  coorelation threshold\",p_error))\n",
    "\n",
    "        overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=0,\n",
    "                weight_over=1,\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "                positive_threshold = 0 ,\n",
    "                negative_threshold = negative_correlation\n",
    "            )\n",
    "        print(f\"overestimate_error using 0% positive and  {correlation_threshold}%  negative  coorelation threshold {overestimate_error} \")\n",
    "        overestimate_error_list.append((f\"overestimate_erro using 0% positive and  {correlation_threshold}%  negative  coorelation threshold\",overestimate_error))\n",
    "        underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=0,\n",
    "                predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                positive_threshold = 0,\n",
    "                negative_threshold = negative_correlation\n",
    "            )\n",
    "        print(f\"underestimate_error using 0% positive and  {correlation_threshold}%  negative  coorelation threshold {underestimate_error}\")\n",
    "        underestimate_error_list.append((f\"underestimate_error using 0% positive and  {correlation_threshold}%  negative  coorelation threshold\",underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusions\n",
    "in fact it didnt seem to work out as planned as it worsen the performance in comparison to the previous algorithm  in all of the metrics. Therefore we shall abandon this idea, anyway it was worth checking out definitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixed neighbourhood size of k most similar neighbours\n",
    "The idea with the negative coorelations didnt really work therefore  we want to further explore the user-based rating prediction basing on the positively coorelated users, hovever this time we wont take all users above some threshold but instead the k most similar users with our UserId and we will create the weighted average of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 5 strongest positively coorelated neighbours 0.6189454443760426\n",
      "RMSE using 5 strongest positively coorelated neighbours 0.8134962988471877\n",
      "Ma at p=10 using 5 strongest positively coorelated neighbours 0.543015873015873\n",
      "overestimate_error using  5 strongest positively coorelated neighbours  0.7107882585333297 \n",
      "underestimate_error using  5 strongest positively coorelated neighbours  0.5964032868467976\n",
      "MAE using 15 strongest positively coorelated neighbours 0.6031413821118339\n",
      "RMSE using 15 strongest positively coorelated neighbours 0.7939928210640812\n",
      "Ma at p=10 using 15 strongest positively coorelated neighbours 0.4970634920634921\n",
      "overestimate_error using  15 strongest positively coorelated neighbours  0.7272806141227194 \n",
      "underestimate_error using  15 strongest positively coorelated neighbours  0.5787871031982827\n",
      "MAE using 25 strongest positively coorelated neighbours 0.5981621909345929\n",
      "RMSE using 25 strongest positively coorelated neighbours 0.7906557039685967\n",
      "Ma at p=10 using 25 strongest positively coorelated neighbours 0.485952380952381\n",
      "overestimate_error using  25 strongest positively coorelated neighbours  0.7353569459911625 \n",
      "underestimate_error using  25 strongest positively coorelated neighbours  0.57085036647656\n",
      "MAE using 50 strongest positively coorelated neighbours 0.6003440458652951\n",
      "RMSE using 50 strongest positively coorelated neighbours 0.7924685807405953\n",
      "Ma at p=10 using 50 strongest positively coorelated neighbours 0.500952380952381\n",
      "overestimate_error using  50 strongest positively coorelated neighbours  0.7383173833826344 \n",
      "underestimate_error using  50 strongest positively coorelated neighbours  0.5726464490431844\n"
     ]
    }
   ],
   "source": [
    "for neighbours_size in [ 5,15,25,50]:\n",
    "        mae = evaluate_recommendation_quality_general(\n",
    "            transformed_df=transformed_df,\n",
    "            num_splits=10,\n",
    "            predict_function=predict_scores,  # Function to predict scores\n",
    "            userId=610,\n",
    "            k = neighbours_size\n",
    "             # Pass the threshold as a keyword argument\n",
    "        )\n",
    "        rmse = evaluate_recommendation_quality_rmse_general(\n",
    "                transformed_df=transformed_df,\n",
    "                predict_function=predict_scores,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = neighbours_size\n",
    "            )\n",
    "        print(f\"MAE using {neighbours_size} strongest positively coorelated neighbours {mae}\")\n",
    "        mae_list.append((f\"MAE using {neighbours_size} strongest positively coorelated neighbours\",mae))\n",
    "        \n",
    "        print(f\"RMSE using {neighbours_size} strongest positively coorelated neighbours {rmse}\")\n",
    "        rmse_list.append((f\"Rmse using {neighbours_size} strongest positively coorelated neighbours\",rmse))\n",
    "\n",
    "        p_error = evaluate_recommendation_quality_at_p(\n",
    "                p=10,\n",
    "                transformed_df=transformed_df,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores,  # Function to predict scores\n",
    "                userId=610,\n",
    "                k = neighbours_size\n",
    "            )\n",
    "        print(f\"Ma at p=10 using {neighbours_size} strongest positively coorelated neighbours {p_error}\")\n",
    "        perror_list.append((f\"p_error using {neighbours_size} strongest positively coorelated neighbours\",p_error))\n",
    "\n",
    "        overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=0,\n",
    "                weight_over=1,\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores,  # Function to predict scores\n",
    "                k = neighbours_size\n",
    "            )\n",
    "        print(f\"overestimate_error using  {neighbours_size} strongest positively coorelated neighbours  {overestimate_error} \")\n",
    "        overestimate_error_list.append((f\"overestimate_error using {neighbours_size} strongest positively coorelated neighbours\",overestimate_error))\n",
    "        underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=0,\n",
    "                predict_function=predict_scores,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = neighbours_size\n",
    "            )\n",
    "        print(f\"underestimate_error using  {neighbours_size} strongest positively coorelated neighbours  {underestimate_error}\")\n",
    "        underestimate_error_list.append((f\"underestimate_error using {neighbours_size} strongest positively coorelated neighbours\",underestimate_error))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusions\n",
    "It seems to have yielded more promising results than the previous idea of the negative coorelations incorporation. However the improvements are not anyhow staggering. The RMSE did not improve in comparison to taking all positively coorelated users , however the mae and mae at p=10  have now improved , but it also depends on the size of the neighbourhood, so this parameter must be choosen wisely. Overall the best performance have been achieved with 25 most similar users. Unfortunately, maybe due to taking less amount of data into consideration when predictiong rating for each film , the overestimation_error and underestimation error differ quite a lot -the algorithm tends to overestimate , it  could also be beneficial for a user who has a lot of time for watching movies and would not mind some false positive high ratings, but in general it is not the best outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change of approach\n",
    "So until now we focused on the coorelation of our user to the other users and ratings for these other users for this film, it seems resonable, but in fact it does not account for one important thing: The personal tendency of users to rate films high or low . So some users may be grumpy in general whereas others might be indulgent overall in their grading, so to account for that we need to look how the rating for the film that we are currently considering deviates for the given users from their rating means, this way we account for the individual tendencies to rank high or low. Therefore our recommender system shall be in fact more versatile and accurate. We will use previously discovered threshold of 25 strongest positively correlated users in this case as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low 0.5400134448817265\n",
      "RMSE using 25 strongest positively coorelated neighbours  accounting for the personal  tendencies high/low0.6820267320787797\n",
      "Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low 0.47314467607295896\n",
      "overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  0.5110370673816587 \n",
      "underestimate_error using  50 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  0.5572389916468498 \n"
     ]
    }
   ],
   "source": [
    "mae = evaluate_recommendation_quality_general(\n",
    "            transformed_df=transformed_df,\n",
    "            num_splits=10,\n",
    "            predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "            userId=610,\n",
    "            k = 25\n",
    "             # Pass the threshold as a keyword argument\n",
    "        )\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "                transformed_df=transformed_df,\n",
    "                predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low {mae}\")\n",
    "mae_list.append((f\"MAE using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",mae))\n",
    "        \n",
    "print(f\"RMSE using 25 strongest positively coorelated neighbours  accounting for the personal  tendencies high/low{rmse}\")\n",
    "rmse_list.append((f\"Rmse using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "                p=10,\n",
    "                transformed_df=transformed_df,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "                userId=610,\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low {p_error}\")\n",
    "perror_list.append((f\"p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",p_error))\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=0,\n",
    "                weight_over=1,\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  {overestimate_error} \")\n",
    "overestimate_error_list.append((f\"overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",overestimate_error))\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=0,\n",
    "                predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = 25 \n",
    "            )\n",
    "print(f\"underestimate_error using  {neighbours_size} strongest positively coorelated neighbours accounting for the personal  tendencies high/low  {underestimate_error} \")\n",
    "underestimate_error_list.append((f\"underestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",underestimate_error))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusions \n",
    "And it turned to be a very good idea, the mae decreased visibly and the RMse decreased  even more significantly from the previous best of around 0.79 to 0.68 now. The overestimate_error and underestimate error are very similar and significantly lower than they used to be what is also a very good outcome. The error at 10 best ratings also decreased, but only a tinge in comparison to its previous best value. Generally this idea seem to perform the best so far, as it minimizes the error in all the metrics, yielding it not only good but also versatile approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item analysis\n",
    "Lastly , why dont we analyze the items as well to improve our algorithm, the idea is that we do virtually the same as in the previous algorithm , however at the end of calculating this weighted deviation for our (userId,movie) tupple, we check if the score was equivocal, i.e if the score was close to the  line of change , to clarify lets think about an example: lets say average weighted score says 3.2  then it would be rounded to 3 because this is the closest viable rating to the result, but on the other hand is also not that far from 3.5  .In this case we check for the mean value for this item(movie) if it is above 3.5 we set our average to 3.5. Analogously we can round down with this approach. It seems  a great idea because it adds the item analysis aspect to the algorithm in the places where the user behaviour analysis is  torn between two ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity 0.5357688482689216\n",
      "RMSE using 25 strongest positively coorelated neighbours  accounting for the personal  tendencies high/low0.7071960837101245\n",
      "Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity 0.48492063492063486\n",
      "overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity  0.7553065406303409 \n",
      "underestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity 0.4248038154063024 \n"
     ]
    }
   ],
   "source": [
    "mae = evaluate_recommendation_quality_general(\n",
    "            transformed_df=transformed_df,\n",
    "            num_splits=10,\n",
    "            predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "            userId=610,\n",
    "            k = 25\n",
    "             # Pass the threshold as a keyword argument\n",
    "        )\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "                transformed_df=transformed_df,\n",
    "                predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity {mae}\")\n",
    "mae_list.append((f\"MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity\",mae))\n",
    "        \n",
    "print(f\"RMSE using 25 strongest positively coorelated neighbours  accounting for the personal  tendencies high/low{rmse}\")\n",
    "rmse_list.append((f\"Rmse using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity\",rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "                p=10,\n",
    "                transformed_df=transformed_df,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "                userId=610,\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity {p_error}\")\n",
    "perror_list.append((f\"p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity\",p_error))\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=0,\n",
    "                weight_over=1,\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "                k = 25\n",
    "            )\n",
    "print(f\"overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity  {overestimate_error} \")\n",
    "overestimate_error_list.append((f\"overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity\",overestimate_error))\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "                transformed_df=transformed_df,\n",
    "                weight_under=1,\n",
    "                weight_over=0,\n",
    "                predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "                userId=610,\n",
    "                num_splits=10,\n",
    "                k = 25 \n",
    "            )\n",
    "print(f\"underestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity {underestimate_error} \")\n",
    "underestimate_error_list.append((f\"underestimate_error using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity\",underestimate_error))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusions\n",
    "The difference in comparison to the previous method is not very striking. The mae is slightly better here, however on the other hand the RMse is slightly  worse , also the mae at 10 best ratings is slightly worse. The underestimate_error is low as never before, meaning this system tends to  overestimate, and probably would be the best choices for user not being very picky but wanting to have a broad set of nice rated films to watch. In general the idea seem quite interesting but probably it requires more testing in order for it to make a good difference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results comparison and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE  scores:\n",
      "[('MAE using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity', 0.5357688482689216), ('MAE using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.5400134448817265), ('MAE using 25 strongest positively coorelated neighbours', 0.5981621909345929), ('MAE using 50 strongest positively coorelated neighbours', 0.6003440458652951), ('MAE using 0% positive coorelation threshold', 0.6021952781071522), ('MAE using 15 strongest positively coorelated neighbours', 0.6031413821118339), ('MAE using 0% positive and  0.6%  negative  coorelation threshold', 0.604099744608393), ('MAE using 0% positive and  0.6%  negative  coorelation threshold', 0.6060534511840754), ('MAE using 0.2% positive coorelation threshold', 0.606241746389952), ('MAE using 0% positive and  0.6%  negative  coorelation threshold', 0.6113749808614168), ('MAE using 5 strongest positively coorelated neighbours', 0.6189454443760426), ('MAE using 0.4% positive coorelation threshold', 0.6355510566787939), ('predict_baseline', 0.6701644157369347), ('MAE using 0.6% positive coorelation threshold', 0.737954359048584)]\n",
      "\n",
      "\n",
      "rmse  scores:\n",
      "[('Rmse using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.6820267320787797), ('Rmse using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity', 0.7071960837101245), ('Rmse using 25 strongest positively coorelated neighbours', 0.7906557039685967), ('Rmse using 50 strongest positively coorelated neighbours', 0.7924685807405953), ('Rmse using 0% positive coorelation threshold', 0.7939195306055609), ('Rmse using 15 strongest positively coorelated neighbours', 0.7939928210640812), ('Rmse using 0.6 positive coorelation threshold', 0.7953086282698761), ('Rmse using 0.6 positive coorelation threshold', 0.7972685194076556), ('Rmse using 0.6 positive coorelation threshold', 0.80136603031125), ('Rmse using 0.2% positive coorelation threshold', 0.802867162923574), ('Rmse using 5 strongest positively coorelated neighbours', 0.8134962988471877), ('Rmse using 0.4% positive coorelation threshold', 0.8408924823724673), ('predict_baseline', 0.8670309338053681), ('Rmse using 0.6% positive coorelation threshold', 0.933130514623205)]\n",
      "\n",
      "\n",
      "p_error at p =10   scores:\n",
      "[('p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.47314467607295896), ('p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity', 0.48492063492063486), ('p_error using 25 strongest positively coorelated neighbours', 0.485952380952381), ('p_error using 15 strongest positively coorelated neighbours', 0.4970634920634921), ('p_error using 50 strongest positively coorelated neighbours', 0.500952380952381), ('Mae at p=10 using 0 positive coorelation threshold', 0.505952380952381), ('p_error using 0% positive and  0.6%  negative  coorelation threshold', 0.505952380952381), ('p_error using 0% positive and  0.6%  negative  coorelation threshold', 0.510952380952381), ('p_error using 0% positive and  0.6%  negative  coorelation threshold', 0.510952380952381), ('Mae at p=10 using 0.2 positive coorelation threshold', 0.5147222222222223), ('Mae at p=10 using 0.4 positive coorelation threshold', 0.5407936507936507), ('p_error using 5 strongest positively coorelated neighbours', 0.543015873015873), ('predict_baseline', 0.675), ('Mae at p=10 using 0.6 positive coorelation threshold', nan)]\n",
      "\n",
      "\n",
      "underestimate errror  scores:\n",
      "[('underestimate_error using 25strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity', 0.4248038154063024), ('underestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.5572389916468498), ('underestimate_error using 25 strongest positively coorelated neighbours', 0.57085036647656), ('underestimate_error using 50 strongest positively coorelated neighbours', 0.5726464490431844), ('p_error using 0% positive coorelation threshold', 0.574598358638441), ('underestimate_error using 0% positive and  0.6%  negative  coorelation threshold', 0.5766572684607717), ('underestimate_error using 15 strongest positively coorelated neighbours', 0.5787871031982827), ('underestimate_error using 0% positive and  0.6%  negative  coorelation threshold', 0.5793087606016811), ('underestimate_error using 0% positive and  0.6%  negative  coorelation threshold', 0.5859043019335467), ('p_error using 0.2% positive coorelation threshold', 0.5873767604119139), ('predict_baseline', 0.5890073252784332), ('underestimate_error using 5 strongest positively coorelated neighbours', 0.5964032868467976), ('p_error using 0.4% positive coorelation threshold', 0.6224698281566787), ('p_error using 0.6% positive coorelation threshold', 0.6739252769765453)]\n",
      "\n",
      "\n",
      "overestimate error scores:\n",
      "[('overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.5110370673816587), ('Overestimate Mae using  0.2% positive coorelation threshold', 0.619356961510738), ('Overestimate Mae using  0% positive coorelation threshold', 0.6211578619909098), ('Overestimate Mae using  0.4% positive coorelation threshold', 0.6449831749158503), ('overestimate_error using 5 strongest positively coorelated neighbours', 0.7107882585333297), ('overestimate_error using 15 strongest positively coorelated neighbours', 0.7272806141227194), ('overestimate_erro using 0% positive and  0.6%  negative  coorelation threshold', 0.7347326854718496), ('overestimate_error using 25 strongest positively coorelated neighbours', 0.7353569459911625), ('overestimate_erro using 0% positive and  0.6%  negative  coorelation threshold', 0.7360172704520868), ('overestimate_error using 50 strongest positively coorelated neighbours', 0.7383173833826344), ('overestimate_erro using 0% positive and  0.6%  negative  coorelation threshold', 0.745418158624428), ('overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low and items specificity', 0.7553065406303409), ('Overestimate Mae using  0.6% positive coorelation threshold', 0.7608267474862302), ('predict_baseline', 0.8334233572346182)]\n"
     ]
    }
   ],
   "source": [
    "mae_list.sort(key = lambda x: x[-1])\n",
    "rmse_list.sort(key = lambda x: x[-1])\n",
    "underestimate_error_list.sort(key = lambda x: x[-1])\n",
    "overestimate_error_list.sort(key = lambda x: x[-1])\n",
    "perror_list.sort(key = lambda x: x[-1])\n",
    "\n",
    "print (\"MAE  scores:\")\n",
    "print(mae_list)\n",
    "print('\\n')\n",
    "print (\"rmse  scores:\")\n",
    "print(rmse_list)\n",
    "print('\\n')\n",
    "print (\"p_error at p =10   scores:\")\n",
    "print(perror_list)\n",
    "print('\\n')\n",
    "print (\"underestimate errror  scores:\")\n",
    "print(underestimate_error_list)\n",
    "print('\\n')\n",
    "print (\"overestimate error scores:\")\n",
    "print(overestimate_error_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We believe that we tested pretty throughly several recommender system approaches, often building up one on the base of another . \n",
    "The main conclusion is that the system basing on the weighted average of deviation from means for 25 most correlated users performs the best .It is definitely the most versatile, performs good in all metrics, and also is simply the best for some metrics, for example MRSE - which is an important measure for this problem. \n",
    "- Additionaly, The item analysis is a very promising idea , because already here a simple approach with conflict resolutions basing on the mean for the given movie  used in case the weighted average of users gave equivocal score , proved promising . So probably after more work and testing some better results could be achieved using item-based recommender system combined with user-based. Regarding this matter, we have also tried to combine the user-based recommender based on correlation with the item-based recommender based on cosine-similarity between movies. However it turned out to be even more computationaly expensive than the user based recommender systems therefore we decided to resign of that.\n",
    "- The baseline approach , very simple and cheap system, seems not to perform that bad in terms of mae. But for other, maybe more informative measures , it is clearly worse. Still it is not a bad idea to use it for comparison or as a baseline for predictions as it is not worse  by 'order of magnitude' than the other more sophisticated approaches\n",
    "- Our approach to make use of the negatively coorelated users did not succed. Maybe the idea behind the algorithm, despite looking resonable,  was in fact invalid . Introducing this idea in our systems did only worsen performance in all of the metrics, However it may be only the issue of  invalid logic not issue of using the negative coorelation per se."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
