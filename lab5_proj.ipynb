{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgBD1kjpRPFj"
   },
   "source": [
    "# Recommendation system\n",
    "The idea of the project is to predict ratings of unwatched movies  for userId 610 in order to be able to recommend movies to this user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej2NXtHGRPFk"
   },
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MNC8aGXGRPFk",
    "outputId": "89cdf949-50f2-46c9-8e53-42ea618c6423"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(156068)\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "df = pd.read_csv('./ratings.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0ZNsO3hRPFl"
   },
   "source": [
    "### Transforming the dataframe into a convinient form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "HnLH7qWeRPFm",
    "outputId": "d1ebab77-a190-4b82-abc2-23384650432f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "movieId                                                    ...                  \n",
       "1        4.0  NaN  NaN  NaN  4.0  NaN  4.5  NaN  NaN  NaN  ...  4.0  NaN  4.0   \n",
       "2        NaN  NaN  NaN  NaN  NaN  4.0  NaN  4.0  NaN  NaN  ...  NaN  4.0  NaN   \n",
       "3        4.0  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "4        NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "5        NaN  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "\n",
       "userId   604  605  606  607  608  609  610  \n",
       "movieId                                     \n",
       "1        3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
       "2        5.0  3.5  NaN  NaN  2.0  NaN  NaN  \n",
       "3        NaN  NaN  NaN  NaN  2.0  NaN  NaN  \n",
       "4        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "5        3.0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userId = 610\n",
    "transformed_df = df.pivot(index='movieId', columns='userId', values='rating')\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_bAg8eXRPFm"
   },
   "source": [
    "### Correlation finding methods\n",
    "Below are the functions used to find correlations between users based on the mutual movies they have watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d0tZpS2VRPFm"
   },
   "outputs": [],
   "source": [
    "'''the coorelation finding method which just finds the pearson coorelation between the  given user and all other users ,\n",
    "if they have at least 2 movies in common\n",
    "and if neither given user nor other user ratings for common films are constant array '''\n",
    "def find_correlation(transformed_df:pd.DataFrame,userId=610):\n",
    "    user_column = transformed_df[userId].copy()\n",
    "    user_column.dropna(inplace=True)\n",
    "    correlations = dict()\n",
    "\n",
    "    for other_user in transformed_df.drop(userId,axis=1).columns:\n",
    "        # Cannot check correlations between arrays with 1 element\n",
    "        common_ratings = user_column.index.intersection(transformed_df[other_user].dropna().index)\n",
    "        if len(common_ratings)>1:\n",
    "            common_ratings = list(common_ratings)\n",
    "\n",
    "            # Cannot check correlation with array of all elements with the same value, since the concept of correlation does not apply there\n",
    "            if user_column[common_ratings].nunique() > 1 and transformed_df[other_user][common_ratings].nunique() > 1:\n",
    "                corr, _ = pearsonr(user_column[common_ratings], transformed_df[other_user][common_ratings])\n",
    "                correlations[other_user] = corr\n",
    "                continue\n",
    "\n",
    "        correlations[other_user] = np.nan\n",
    "\n",
    "    correlation_df = pd.Series(correlations)\n",
    "\n",
    "    return correlation_df\n",
    "\n",
    "'''the coorelation finding function as above,\n",
    " however  the coorelation is found only if the user and other user have at least threshold films in common'''\n",
    "def find_correlation_with_common_films_threshold(transformed_df:pd.DataFrame,threshold=5,userId=610):\n",
    "    user_column = transformed_df[userId].copy()\n",
    "    user_column.dropna(inplace=True)\n",
    "    correlations = dict()\n",
    "\n",
    "    for other_user in transformed_df.drop(userId,axis=1).columns:\n",
    "        # Cannot check correlations between arrays with 1 element\n",
    "        common_ratings = user_column.index.intersection(transformed_df[other_user].dropna().index)\n",
    "        if len(common_ratings)>threshold:\n",
    "            common_ratings = list(common_ratings)\n",
    "            # Cannot check correlation with array of all elements with the same value, since the concept of correlation does not apply there\n",
    "            if user_column[common_ratings].nunique() > 1 and transformed_df[other_user][common_ratings].nunique() > 1:\n",
    "                corr, _ = pearsonr(user_column[common_ratings], transformed_df[other_user][common_ratings])\n",
    "                correlations[other_user] = corr\n",
    "                continue\n",
    "\n",
    "        correlations[other_user] = np.nan\n",
    "\n",
    "    correlation_df = pd.Series(correlations)\n",
    "\n",
    "    return correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAJknDiNRPFm"
   },
   "source": [
    "### Movie recommender systems\n",
    "Here, different approaches to movie recommender systems can be found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dIeZElnERPFm"
   },
   "outputs": [],
   "source": [
    "def predict_scores(transformed_df: pd.DataFrame, userId=610, k=10,split = None):\n",
    "    '''we find up to k most coorelated users with our userID  that have rated given movie not seen by our userID\n",
    "        and calculate rating  as a weighted average\n",
    "        of their ratings for this film. up to k because it may happen that there are less than k users\n",
    "        with positive correlation with our userId who watched\n",
    "        given movie'''\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    sorted_correlations = sorted_correlations[sorted_correlations > 0]\n",
    "    sorted_transposed_df = transposed_df.loc[sorted_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            top_users_indices = sorted_correlations.index.intersection(valid_ratings.index)[:k]\n",
    "            if top_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = valid_ratings[top_users_indices].dot(user_correlations[top_users_indices]) / user_correlations[top_users_indices].sum()\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "def predict_scores_no_baseline(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using top `k` most correlated users. Rating for tuple (userId, movie) predicted as mean of ratings for our userID for all movies +\n",
    "    weighted deviation from the mean for given movie of the k most correlated users.\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    user_means = transposed_df.mean(axis=1, skipna=True)\n",
    "    user_mean_target = user_means[userId]\n",
    "    predictions_df = transposed_df.copy()\n",
    "\n",
    "    for movie in transposed_df.columns:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "        ratings_for_movie = transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_sorted = sorted_correlations.index.intersection(valid_ratings.index)\n",
    "            top_k_users = valid_users_sorted[:k]\n",
    "            if len(top_k_users) == 0:\n",
    "                continue\n",
    "            weighted_deviation_sum = 0\n",
    "            total_weight = 0\n",
    "\n",
    "            for user in top_k_users:\n",
    "                correlation = sorted_correlations[user]\n",
    "                user_mean = user_means[user]\n",
    "                user_rating = ratings_for_movie[user]\n",
    "                deviation = user_rating - user_mean\n",
    "                weighted_deviation_sum += correlation * deviation\n",
    "                total_weight += abs(correlation)\n",
    "            if total_weight > 0:\n",
    "                weighted_deviation = weighted_deviation_sum / total_weight\n",
    "                predicted_score = user_mean_target + weighted_deviation\n",
    "                predictions_df.loc[userId, movie] = predicted_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "def predict_scores_with_item_correction(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    border=0.8,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    works similar to the predictor above, only at the end it adjusts the  rating basing on the mean rating for given film in case\n",
    "    the rating is close to the border of ratings\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    user_means = transposed_df.mean(axis=1, skipna=True)\n",
    "    item_means = transformed_df.mean(axis=1, skipna=True)\n",
    "    user_mean_target = user_means[userId]\n",
    "    predictions_df = transposed_df.copy()\n",
    "\n",
    "    for movie in transposed_df.columns:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "        ratings_for_movie = transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_sorted = sorted_correlations.index.intersection(valid_ratings.index)\n",
    "            top_k_users = valid_users_sorted[:k]\n",
    "            if len(top_k_users) == 0:\n",
    "                continue\n",
    "            weighted_deviation_sum = 0\n",
    "            total_weight = 0\n",
    "\n",
    "            for user in top_k_users:\n",
    "                correlation = sorted_correlations[user]\n",
    "                user_mean = user_means[user]\n",
    "                user_rating = ratings_for_movie[user]\n",
    "                deviation = user_rating - user_mean\n",
    "                weighted_deviation_sum += correlation * deviation\n",
    "                total_weight += abs(correlation)\n",
    "\n",
    "            weighted_deviation = weighted_deviation_sum / total_weight if total_weight > 0 else 0\n",
    "            item_mean = item_means[movie] if movie in item_means else 0\n",
    "            if item_mean >= round(user_mean_target*2)/2 and 2*user_mean_target - int(2*user_mean_target) >= border:\n",
    "                correction = 0.5\n",
    "            elif item_mean <= round(user_mean_target*2)/2 and 2*user_mean_target - int(2*user_mean_target) <= (1-border):\n",
    "                correction = - 0.5\n",
    "            else:\n",
    "                correction = 0\n",
    "\n",
    "            predicted_score = user_mean_target + weighted_deviation + correction\n",
    "            predicted_score = round(predicted_score * 2) / 2\n",
    "            predicted_score = max(0.5,predicted_score)\n",
    "            predictions_df.loc[userId, movie] = predicted_score\n",
    "\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "def predict_baseline(transformed_df: pd.DataFrame, userId=610, k=1,split=None):\n",
    "    \"\"\"\n",
    "    Predict baseline scores for movies the user has not rated.\n",
    "    mean of user ratings of this user + movie ratings for this movie\n",
    "    \"\"\"\n",
    "    predictions_df = transformed_df.copy()\n",
    "    for movie in transformed_df.index:\n",
    "        if split is not None and movie not in split:\n",
    "            continue\n",
    "\n",
    "        if pd.isna(transformed_df.loc[movie, userId]):\n",
    "            user_ratings = transformed_df[userId].dropna()\n",
    "            movie_ratings = transformed_df.loc[movie].dropna()\n",
    "            total_ratings_sum = user_ratings.sum() + movie_ratings.sum()\n",
    "            total_ratings_count = len(user_ratings) + len(movie_ratings)\n",
    "            if total_ratings_count > 0:\n",
    "                baseline_prediction = total_ratings_sum / total_ratings_count\n",
    "                predictions_df.loc[movie, userId] = round(baseline_prediction * 2) / 2  # Round to nearest 0.5\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "def predict_scores_with_threshold(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=1,\n",
    "    threshold=0.8,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using all correlated users above a specified threshold.\n",
    "    weighted average of these users  rating for this film . Weights are the coorelation\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    filtered_correlations = user_correlations[user_correlations > threshold]\n",
    "    sorted_transposed_df = transposed_df.loc[filtered_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            valid_users_indices =filtered_correlations.index.intersection(valid_ratings.index)\n",
    "            if valid_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = (\n",
    "                valid_ratings[valid_users_indices].dot(filtered_correlations[valid_users_indices]) /\n",
    "                filtered_correlations[valid_users_indices].sum()\n",
    "            )\n",
    "\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "def predict_scores_with_threshold_negative(\n",
    "    transformed_df: pd.DataFrame,\n",
    "    userId=610,\n",
    "    k=1,\n",
    "    positive_threshold=0,\n",
    "    negative_threshold=0,\n",
    "    split=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict scores using all correlated users above a specified threshold for positive correlations,\n",
    "    and include negatively correlated users below a specified threshold with adjusted contribution.\n",
    "    \"\"\"\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation(transformed_df, userId)\n",
    "    positive_correlations = user_correlations[user_correlations > positive_threshold]\n",
    "    negative_correlations = user_correlations[\n",
    "        (user_correlations < 0) & (user_correlations.abs() > negative_threshold)\n",
    "    ]\n",
    "    sorted_transposed_df = transposed_df.loc[\n",
    "        positive_correlations.index.union(negative_correlations.index)\n",
    "    ]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            positive_users = positive_correlations.index.intersection(valid_ratings.index)\n",
    "            negative_users = negative_correlations.index.intersection(valid_ratings.index)\n",
    "            weighted_sum = 0\n",
    "            total_weight = 0\n",
    "            if not positive_users.empty:\n",
    "                weighted_sum += (\n",
    "                    valid_ratings[positive_users].dot(positive_correlations[positive_users])\n",
    "                )\n",
    "                total_weight += positive_correlations[positive_users].sum()\n",
    "\n",
    "            for neg_user in negative_users:\n",
    "                neg_rating = valid_ratings[neg_user]\n",
    "                if neg_rating < 2 or neg_rating > 4:\n",
    "                    adjusted_rating = 6 - neg_rating\n",
    "                    weight = abs(negative_correlations[neg_user])\n",
    "                    weighted_sum += adjusted_rating * weight\n",
    "                    total_weight += weight\n",
    "\n",
    "            if total_weight == 0:\n",
    "                continue\n",
    "\n",
    "            weighted_avg = weighted_sum / total_weight\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()\n",
    "\n",
    "def predict_scores_strict_correlation(transformed_df: pd.DataFrame, userId=610, k=10,correlation_neighbours=5,split = None):\n",
    "    '''function working the same as predict_scores described above, with only difference , that coorelation are defined  only for the users\n",
    "    having at least coorelation_neighbours mutual watched films with userId '''\n",
    "    transposed_df = transformed_df.transpose()\n",
    "    user_correlations = find_correlation_with_common_films_threshold(transformed_df,correlation_neighbours,userId)\n",
    "    sorted_correlations = user_correlations.sort_values(ascending=False)\n",
    "    sorted_correlations = sorted_correlations[sorted_correlations > 0]\n",
    "    sorted_transposed_df = transposed_df.loc[sorted_correlations.index]\n",
    "    if userId not in sorted_transposed_df.index:\n",
    "        sorted_transposed_df.loc[userId] = transposed_df.loc[userId]\n",
    "    predictions_df = sorted_transposed_df.copy()\n",
    "    for movie in sorted_transposed_df.columns:\n",
    "        if split is not None:\n",
    "            if movie not in split:\n",
    "                continue\n",
    "        ratings_for_movie = sorted_transposed_df[movie]\n",
    "        if pd.isna(ratings_for_movie[userId]):\n",
    "            valid_ratings = ratings_for_movie.dropna()\n",
    "            top_users_indices = sorted_correlations.index.intersection(valid_ratings.index)[:k]\n",
    "            if top_users_indices.empty:\n",
    "                continue\n",
    "            weighted_avg = valid_ratings[top_users_indices].dot(user_correlations[top_users_indices]) / user_correlations[top_users_indices].sum()\n",
    "            valid_score = round(weighted_avg * 2) / 2\n",
    "            predictions_df.loc[userId, movie] = valid_score\n",
    "    return predictions_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPvopAqAfwcX"
   },
   "source": [
    "### Correlations between user 610 and other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "0sP7OrKifwA_",
    "outputId": "f06ce488-fc7b-4818-e527-84bcf6dc15e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576    1.000000\n",
       "545    1.000000\n",
       "442    1.000000\n",
       "158    0.911322\n",
       "92     0.903696\n",
       "         ...   \n",
       "54    -0.759555\n",
       "388   -0.866025\n",
       "383   -0.870388\n",
       "194   -0.944911\n",
       "250   -1.000000\n",
       "Length: 598, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = find_correlation(transformed_df, 610)\n",
    "correlations = correlations.sort_values(ascending=False)\n",
    "correlations = correlations.dropna()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkDAlH9Bk2Po"
   },
   "source": [
    "Seems that there are actually 3 users that have a perfect correlation. Let's investigate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "Xk9a4fhdk-cu",
    "outputId": "501d2276-d317-4319-976f-8547c3c3a773"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; align-items: top; gap: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>576</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>545</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33794</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>442</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_ids_with_correlation_1 = correlations[correlations == 1.0].index\n",
    "\n",
    "html = '<div style=\"display: flex; align-items: top; gap: 32px\">'\n",
    "for id in user_ids_with_correlation_1:\n",
    "    user_ratings = transformed_df[id].dropna().to_frame()\n",
    "    user_610_ratings = transformed_df[610].loc[user_ratings.index].to_frame()\n",
    "    combined_df = pd.concat([user_ratings, user_610_ratings], axis=1)\n",
    "    combined_df = combined_df.dropna()\n",
    "    df_html = combined_df.to_html()\n",
    "    html += df_html\n",
    "html += '</div>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT66TaqWluZi",
    "outputId": "6a557602-7c53-4f0a-9842-f252b4e739ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df[610][1261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q31SMtNKpYDI"
   },
   "source": [
    "Looks like these rated very few movies that were also rated by user 610, but for those movies that were rated by both users, the correlation was perfect (this does not mean identical ratings, as we are using pearson correlation).\n",
    "\n",
    "Let us find users that have at least 5 movies in common with user 610 and that have the highest correlation with that user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Bao11NJNsmAD",
    "outputId": "314c0584-c827-4f39-888d-51c00623086e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120    0.879537\n",
       "463    0.823180\n",
       "138    0.822192\n",
       "494    0.811761\n",
       "13     0.796969\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_with_treshold = find_correlation_with_common_films_threshold(transformed_df,5,610)\n",
    "correlations_with_treshold = correlations_with_treshold.sort_values(ascending=False)\n",
    "correlations_with_treshold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi7WYwG8szIJ"
   },
   "source": [
    "These results look more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpxsttvyRPFn"
   },
   "source": [
    "### Evaluation of the performance of prediction methods\n",
    "Frameworks for evaluating recommender systems are presented here. All evaluations are based on 10-fold cross-validation on user ID 610, ensuring consistent splits across every run of the evaluation methods. This guarantees that comparisons between different recommender systems are reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "twItvelZRPFo"
   },
   "outputs": [],
   "source": [
    "def evaluate_recommendation_quality_general(\n",
    "    transformed_df,\n",
    "    predict_function,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    General evaluation function for recommendation quality using MAE.\n",
    "    \"\"\"\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index\n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)\n",
    "        mae = np.abs(predicted_scores[valid_indices] - actual_scores[valid_indices]).mean()\n",
    "\n",
    "        mae_list.append(mae)\n",
    "\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def evaluate_recommendation_quality_at_p(\n",
    "    transformed_df,\n",
    "    predict_function,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    p=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for evaluating the Mae at p  highest rated items.\n",
    "    \"\"\"\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index\n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        predicted_scores.sort_values(ascending=False)\n",
    "        predicted_scores = predicted_scores[:p]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)\n",
    "        mae = np.abs(predicted_scores[valid_indices] - actual_scores[valid_indices]).mean()\n",
    "        mae_list.append(mae)\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def evaluate_recommendation_quality_rmse_general(\n",
    "    transformed_df,\n",
    "    predict_function,\n",
    "    userId=610,\n",
    "    k=10,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    General evaluation function for recommendation quality using RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index\n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    rmse_list = []\n",
    "\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan, userId=userId, k=k, split=split, **predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)\n",
    "        rmse = np.sqrt(((predicted_scores[valid_indices] - actual_scores[valid_indices]) ** 2).mean())\n",
    "        rmse_list.append(rmse)\n",
    "    return np.mean(rmse_list)\n",
    "\n",
    "def evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df,\n",
    "    predict_function,\n",
    "    weight_under=1,\n",
    "    weight_over=1,\n",
    "    userId=610,\n",
    "    k=20,\n",
    "    num_splits=10,\n",
    "    **predict_function_kwargs\n",
    "):\n",
    "    '''function used to evalue the overestimation error and underestimation error'''\n",
    "    original_ratings = transformed_df.loc[:, userId].dropna()\n",
    "    rated_movies = original_ratings.index\n",
    "    splits = np.array_split(rated_movies, num_splits)\n",
    "    mae_list = []\n",
    "    for split in splits:\n",
    "        df_with_nan = transformed_df.copy()\n",
    "        df_with_nan.loc[split, userId] = np.nan\n",
    "        predicted_df = predict_function(df_with_nan,userId=userId, k=k, split=split,**predict_function_kwargs)\n",
    "        predicted_scores = predicted_df.loc[split, userId]\n",
    "        actual_scores = original_ratings[split]\n",
    "        valid_indices = ~np.isnan(predicted_scores) & ~np.isnan(actual_scores)\n",
    "        error_array = predicted_scores[valid_indices] - actual_scores[valid_indices]\n",
    "        underpredicted_error = error_array[error_array<=0]\n",
    "        overpredicted_error = error_array[error_array>0]\n",
    "        underpredicted_error = np.abs(underpredicted_error)\n",
    "        underpredicted_error  = underpredicted_error * weight_under\n",
    "        overpredicted_error = overpredicted_error * weight_over\n",
    "        weighted_mae = (sum(underpredicted_error) + sum(overpredicted_error)) / (len(underpredicted_error)*weight_under + len(overpredicted_error)*weight_over)\n",
    "        mae_list.append(weighted_mae)\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUGwTBBmRPFo"
   },
   "source": [
    "### Description of testing\n",
    "\n",
    "We tested several movie recommender systems, which are described step by step along with the conclusions drawn from the results. To evaluate the prediction accuracy of the systems, we devised several evaluation methods. All methods are based on 10-fold cross-validation on user ID 610. This involves running 10 iterations of the recommender systems, each using 9/10 of the records for user ID 610 (along with all records for other users) to predict the scores for deliberately erased data for user ID 610. We calculate the error for each iteration and take the mean error across all 10 runs.\n",
    "\n",
    "While using data from other users could provide more insights, our task specifically focuses on predicting ratings for movies for user ID 610. Therefore, the evaluation is solely based on the algorithms' errors for this user ID. When creating the splits for 10-fold validation, we do not shuffle the data, as it is not sorted by rating for user ID 610. Consistent splits are used across evaluations to enable fair comparisons between different recommender systems.\n",
    "\n",
    "We employ several error evaluation methods, including:\n",
    "\n",
    "- **MAE (Mean Absolute Error):** A straightforward and easy-to-interpret measure of prediction error.\n",
    "- **RMSE (Root Mean Squared Error):** A measure that penalizes larger errors more heavily, making it well-suited for this purpose.\n",
    "- **Weighted MAE:** This approach accounts for the user's preferences. For instance:\n",
    "  - If the user is selective and values precision in high ratings, overestimation errors may be more detrimental.\n",
    "  - Conversely, if the user watches many movies and values variety, underestimating high ratings (false negatives) may be more problematic.\n",
    "  To address this, we calculate the MAE with weights applied to overestimation and underestimation errors, allowing us to analyze which systems perform best for high precision or high recall in high ratings. To isolate each type of error, we use weights such as (1,0) for overestimation errors only and (0,1) for underestimation errors only.\n",
    "- **MAE for Top-K Rated Movies:** This measure focuses on the K highest-rated movies. The rationale is that users may care less about ratings for movies they wouldn’t watch anyway. This is especially relevant for selective users with limited time, who prioritize avoiding low-quality movies while being less concerned about films they won’t watch.\n",
    "\n",
    "These evaluation methods enable us to comprehensively assess the performance of the recommender systems and identify the most suitable approaches for specific user preferences and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaEOGQB7RPFo"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLyBzT8gRPFp"
   },
   "source": [
    "##### Remark\n",
    "- Although not explicitly mentioned earlier, each system rounds its final prediction to the nearest valid value. This ensures accurate predictions within the given scale, e.g., for the `ratings.csv`, the scale ranges from 0.5 to 5 in increments of 0.5.\n",
    "- When referring to the correlation between users, we always mean Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NNzR0jESRPFp"
   },
   "outputs": [],
   "source": [
    "### we will store best errors, to be able later to seem which system performed best for which purpose\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "perror_list = []\n",
    "overestimate_error_list = []\n",
    "underestimate_error_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HTy0-BuRPFp"
   },
   "source": [
    "### Baseline predictor\n",
    "First, we evaluate the performance of the baseline predictor to establish a benchmark for comparison. The baseline predictor is a simple approach that predicts the score for a given tuple (movie, user ID) by calculating the mean of all ratings for the given movie plus the mean of all ratings for the given user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3TCUXJYRPFp",
    "outputId": "ff1c8fb3-fae4-4184-b85d-51740adb6144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using the baseline prediction 0.6701644157369347\n",
      "RMSE using the baseline prediction 0.8670309338053681\n",
      "Mae at p=10 using the baseline prediction 0.675\n",
      "Overestimate MAE is 0.8334233572346182 \n",
      "Underestimate MAE is 0.5890073252784332 \n"
     ]
    }
   ],
   "source": [
    "mae = evaluate_recommendation_quality_general(\n",
    "    transformed_df=transformed_df,\n",
    "    predict_function=predict_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10 # Pass the threshold as a keyword argument\n",
    ")\n",
    "mae_list.append(('predict_baseline',mae))\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "    transformed_df=transformed_df,\n",
    "    predict_function=predict_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10 # Pass the threshold as a keyword argument\n",
    ")\n",
    "print(f\"MAE using the baseline prediction {mae}\")\n",
    "print(f\"RMSE using the baseline prediction {rmse}\")\n",
    "rmse_list.append(('predict_baseline', rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "    p=10,\n",
    "    transformed_df=transformed_df,\n",
    "    predict_function=predict_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10 # Pass the threshold as a keyword argument\n",
    ")\n",
    "print(f\"Mae at p=10 using the baseline prediction {p_error}\")\n",
    "perror_list.append(('predict_baseline', p_error))\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=0,\n",
    "    weight_over=1,\n",
    "    predict_function=predict_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10 # Pass the threshold as a keyword argument\n",
    ")\n",
    "print(f\"Overestimate MAE is {overestimate_error} \")\n",
    "overestimate_error_list.append(('predict_baseline',overestimate_error))\n",
    "\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=1,\n",
    "    weight_over=0,\n",
    "    userId=610,\n",
    "    predict_function=predict_baseline,  # Function to predict scores\n",
    "    num_splits=10 # Pass the threshold as a keyword argument\n",
    ")\n",
    "print(f\"Underestimate MAE is {underestimate_error} \")\n",
    "underestimate_error_list.append(('predict_baseline',underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYS-jOM5RPFp"
   },
   "source": [
    "#### Conclusions\n",
    "The baseline predictor does not perform terribly in terms of MAE and can serve as a computationally inexpensive starting point. However, one notable issue is that this system, at least with this dataset, is quite biased. The overestimation error is significantly higher than the underestimation error, meaning it tends to overestimate ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8brGPE_URPFq"
   },
   "source": [
    "### User correlation approaches\n",
    "\n",
    "Now that we have a baseline to compare our more sophisticated recommender systems against, we can begin testing them. The first system sets the prediction for the tuple (user ID, movie) as a weighted average of ratings for the given movie from users who have watched the movie and had a positive correlation with the given user, with a correlation value above a certain threshold. We will test several correlation thresholds to determine which one performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjM7jWBKRPFq",
    "outputId": "fc406b0d-9efb-48d0-be26-2f8d37afe266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 0 positive correlation threshold 0.6021952781071522\n",
      "RMSE using 0 positive correlation threshold 0.7939195306055609\n",
      "Mae at p=10 using 0% positive correlation threshold 0.505952380952381\n",
      "Overestimate MAE is 0.6211578619909098 \n",
      "Underestimate_error using 0% positive correlation threshold 0.574598358638441\n",
      "MAE using 0.2 positive correlation threshold 0.606241746389952\n",
      "RMSE using 0.2 positive correlation threshold 0.802867162923574\n",
      "Mae at p=10 using 0.2% positive correlation threshold 0.5147222222222223\n",
      "Overestimate MAE is 0.619356961510738 \n",
      "Underestimate_error using 0.2% positive correlation threshold 0.5873767604119139\n",
      "MAE using 0.4 positive correlation threshold 0.6355510566787939\n",
      "RMSE using 0.4 positive correlation threshold 0.8408924823724673\n",
      "Mae at p=10 using 0.4% positive correlation threshold 0.5407936507936507\n",
      "Overestimate MAE is 0.6449831749158503 \n",
      "Underestimate_error using 0.4% positive correlation threshold 0.6224698281566787\n",
      "MAE using 0.6 positive correlation threshold 0.737954359048584\n",
      "RMSE using 0.6 positive correlation threshold 0.933130514623205\n",
      "Mae at p=10 using 0.6% positive correlation threshold nan\n",
      "Overestimate MAE is 0.7608267474862302 \n",
      "Underestimate_error using 0.6% positive correlation threshold 0.6739252769765453\n"
     ]
    }
   ],
   "source": [
    "for correlation_threshold in [0, 0.2, 0.4, 0.6]:\n",
    "    mae = evaluate_recommendation_quality_general(\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold,\n",
    "        userId=610,\n",
    "        threshold=correlation_threshold\n",
    "    )\n",
    "    rmse = evaluate_recommendation_quality_rmse_general(\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_scores_with_threshold,\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        threshold=correlation_threshold\n",
    "    )\n",
    "    print(f\"MAE using {correlation_threshold} positive correlation threshold {mae}\")\n",
    "    mae_list.append((f\"MAE using {correlation_threshold}% positive correlation threshold\",mae))\n",
    "\n",
    "    print(f\"RMSE using {correlation_threshold} positive correlation threshold {rmse}\")\n",
    "    rmse_list.append((f\"RMSE using {correlation_threshold}% positive correlation threshold\",rmse))\n",
    "\n",
    "    p_error = evaluate_recommendation_quality_at_p(\n",
    "        p=10,\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold,\n",
    "        userId=610,\n",
    "        threshold=correlation_threshold\n",
    "    )\n",
    "    print(f\"Mae at p=10 using {correlation_threshold}% positive correlation threshold {p_error}\")\n",
    "    perror_list.append((f\"MAE at p=10 using {correlation_threshold} positive correlation threshold\", p_error))\n",
    "\n",
    "    overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=1,\n",
    "        weight_over=2,\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold,\n",
    "        threshold=correlation_threshold\n",
    "    )\n",
    "    print(f\"Overestimate MAE is {overestimate_error} \")\n",
    "    overestimate_error_list.append((f\"Overestimate MAE using {correlation_threshold}% positive correlation threshold\", overestimate_error))\n",
    "\n",
    "    underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=1,\n",
    "        weight_over=0,\n",
    "        predict_function=predict_scores_with_threshold,\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        threshold=correlation_threshold\n",
    "    )\n",
    "    print(f\"Underestimate_error using {correlation_threshold}% positive correlation threshold {underestimate_error}\")\n",
    "    underestimate_error_list.append((f\"p_error using {correlation_threshold}% positive correlation threshold\", underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heRHRMSkRPFq"
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "This approach offers some advantages over the baseline method; however, the improvement in MAE is not dramatic, approximately ~10%. The RMSE decreases by an even smaller percentage. Nonetheless, the predictions are noticeably more balanced, with the difference between underestimation and overestimation errors being negligible compared to the baseline. Additionally, the error for the top 10 predicted ratings (p=10) decreases significantly, which is important in real recommender systems, as these are the movies the user is most likely to watch based on predicted ratings.\n",
    "\n",
    "Interestingly, in nearly all metrics, setting the threshold to 0 (thereby including all positively correlated users) yielded the best results. Conversely, limiting the calculation to users with a positive correlation > 0.6 sometimes performed worse than the baseline, with a higher RMSE. This might be due to data sparsity: users with high correlation may have only a few movies (e.g., three) in common with user ID 610, making them less reliable as sources of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oJtCPh5RPFq"
   },
   "source": [
    "### Using negatively correlated users\n",
    "\n",
    "Given the limited availability of data, it would be beneficial to leverage users who are negatively correlated with the current user. There is a significant number of such users, and we devised an algorithm to utilize them effectively. The approach involves using positive correlations as in the previous method but also incorporating users with strong negative correlations (i.e., users whose correlation magnitude exceeds a certain threshold).\n",
    "\n",
    "To include negatively correlated users in the weighted average, we do not directly use their ratings. Instead, we transform their ratings using the formula `rating = 6 - rating`. This adjustment reflects the expectation that if a strongly negatively correlated user rates a film highly, the current user is likely to rate it low, and vice versa. In essence, we create a complement of the original rating.\n",
    "\n",
    "This transformation is applied only to ratings from strongly negatively correlated users in the ranges (0,2] and [4,5). For example, if a strongly negatively correlated user rates a movie as 3, applying the formula would still result in a rating of 3 (`6 - 3 = 3`), which does not align with the expected behavior. To avoid such inconsistencies, these middle-range ratings are excluded from the adjustment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mk6varwwRPFq",
    "outputId": "30070dae-20c6-41c1-dddd-1b1ec469eee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 0% positive and 0.2% negative correlation threshold 0.6113749808614168\n",
      "RMSE using 0% positive and 0.2%  negative  coorelation threshold 0.80136603031125\n",
      "Ma at p=10 using 0% positive and 0.2% negative correlation threshold0.510952380952381\n",
      "overestimate_error using 0% positive and  0.2%  negative  coorelation threshold 0.745418158624428 \n",
      "underestimate_error using 0% positive and 0.2% negative correlation threshold 0.5859043019335467\n",
      "MAE using 0% positive and 0.4% negative correlation threshold 0.6060534511840754\n",
      "RMSE using 0% positive and 0.4%  negative  coorelation threshold 0.7972685194076556\n",
      "Ma at p=10 using 0% positive and 0.4% negative correlation threshold0.510952380952381\n",
      "overestimate_error using 0% positive and  0.4%  negative  coorelation threshold 0.7347326854718496 \n",
      "underestimate_error using 0% positive and 0.4% negative correlation threshold 0.5793087606016811\n",
      "MAE using 0% positive and 0.6% negative correlation threshold 0.604099744608393\n",
      "RMSE using 0% positive and 0.6%  negative  coorelation threshold 0.7953086282698761\n",
      "Ma at p=10 using 0% positive and 0.6% negative correlation threshold0.505952380952381\n",
      "overestimate_error using 0% positive and  0.6%  negative  coorelation threshold 0.7360172704520868 \n",
      "underestimate_error using 0% positive and 0.6% negative correlation threshold 0.5766572684607717\n"
     ]
    }
   ],
   "source": [
    "for negative_correlation in [0.2, 0.4, 0.6]:\n",
    "    mae = evaluate_recommendation_quality_general(\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "        userId=610,\n",
    "        positive_threshold = 0 ,\n",
    "        negative_threshold = negative_correlation\n",
    "          # Pass the threshold as a keyword argument\n",
    "    )\n",
    "    rmse = evaluate_recommendation_quality_rmse_general(\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        positive_threshold = 0 ,\n",
    "        negative_threshold = negative_correlation\n",
    "    )\n",
    "    print(f\"MAE using 0% positive and {negative_correlation}% negative correlation threshold {mae}\")\n",
    "    mae_list.append((f\"MAE using 0% positive and {negative_correlation}% negative correlation threshold\",mae))\n",
    "\n",
    "    print(f\"RMSE using 0% positive and {negative_correlation}%  negative  coorelation threshold {rmse}\")\n",
    "    rmse_list.append((f\"Rmse using {negative_correlation} positive coorelation threshold\",rmse))\n",
    "\n",
    "    p_error = evaluate_recommendation_quality_at_p(\n",
    "        p=10,\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "        userId=610,\n",
    "        positive_threshold = 0 ,\n",
    "        negative_threshold = negative_correlation\n",
    "    )\n",
    "    print(f\"Ma at p=10 using 0% positive and {negative_correlation}% negative correlation threshold{p_error}\")\n",
    "    perror_list.append((f\"p_error using 0% positive and  {negative_correlation}% negative correlation threshold\",p_error))\n",
    "\n",
    "    overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=0,\n",
    "        weight_over=1,\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "        positive_threshold = 0 ,\n",
    "        negative_threshold = negative_correlation\n",
    "    )\n",
    "    print(f\"overestimate_error using 0% positive and  {negative_correlation}%  negative  coorelation threshold {overestimate_error} \")\n",
    "    overestimate_error_list.append((f\"overestimate_erro using 0% positive and {negative_correlation}% negative correlation threshold\", overestimate_error))\n",
    "    underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=1,\n",
    "        weight_over=0,\n",
    "        predict_function=predict_scores_with_threshold_negative,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        positive_threshold = 0,\n",
    "        negative_threshold = negative_correlation\n",
    "    )\n",
    "    print(f\"underestimate_error using 0% positive and {negative_correlation}% negative correlation threshold {underestimate_error}\")\n",
    "    underestimate_error_list.append((f\"underestimate_error using 0% positive and  {negative_correlation}%  negative  coorelation threshold\", underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUIFeFYxRPFr"
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "In practice, this approach did not perform as expected, as it worsened performance compared to the previous algorithm across all metrics. Therefore, we will abandon this idea. Nevertheless, it was certainly worth exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHaX_gMbRPFr"
   },
   "source": [
    "### Fixed neighbourhood size of k most similar neighbours\n",
    "\n",
    "Since the approach with negative correlations did not yield the desired results, we now aim to further explore user-based rating prediction based on the positively correlated users. However, this time, we won’t include all users above a certain threshold. Instead, we will focus on the k most similar users to our User ID. The prediction will be made using a weighted average of these k users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8SinbHMRPFr",
    "outputId": "57fbf94a-5a14-4b59-ce4a-78c22c248a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 5 strongest positively correlated neighbours 0.6189454443760426\n",
      "RMSE using 5 strongest positively correlated neighbours 0.8134962988471877\n",
      "Ma at p=10 using 5 strongest positively coorelated neighbours 0.543015873015873\n",
      "overestimate_error using 5 strongest positively correlated neighbours 0.7107882585333297 \n",
      "underestimate_error using 5 strongest positively correlated neighbours 0.5964032868467976\n",
      "MAE using 15 strongest positively correlated neighbours 0.6031413821118339\n",
      "RMSE using 15 strongest positively correlated neighbours 0.7939928210640812\n",
      "Ma at p=10 using 15 strongest positively coorelated neighbours 0.4970634920634921\n",
      "overestimate_error using 15 strongest positively correlated neighbours 0.7272806141227194 \n",
      "underestimate_error using 15 strongest positively correlated neighbours 0.5787871031982827\n",
      "MAE using 25 strongest positively correlated neighbours 0.5981621909345929\n",
      "RMSE using 25 strongest positively correlated neighbours 0.7906557039685967\n",
      "Ma at p=10 using 25 strongest positively coorelated neighbours 0.485952380952381\n",
      "overestimate_error using 25 strongest positively correlated neighbours 0.7353569459911625 \n",
      "underestimate_error using 25 strongest positively correlated neighbours 0.57085036647656\n",
      "MAE using 50 strongest positively correlated neighbours 0.6003440458652951\n",
      "RMSE using 50 strongest positively correlated neighbours 0.7924685807405953\n",
      "Ma at p=10 using 50 strongest positively coorelated neighbours 0.500952380952381\n",
      "overestimate_error using 50 strongest positively correlated neighbours 0.7383173833826344 \n",
      "underestimate_error using 50 strongest positively correlated neighbours 0.5726464490431844\n"
     ]
    }
   ],
   "source": [
    "for neighbours_size in [5, 15, 25, 50]:\n",
    "    mae = evaluate_recommendation_quality_general(\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores,  # Function to predict scores\n",
    "        userId=610,\n",
    "        k = neighbours_size\n",
    "          # Pass the threshold as a keyword argument\n",
    "    )\n",
    "    rmse = evaluate_recommendation_quality_rmse_general(\n",
    "        transformed_df=transformed_df,\n",
    "        predict_function=predict_scores,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        k = neighbours_size\n",
    "    )\n",
    "    print(f\"MAE using {neighbours_size} strongest positively correlated neighbours {mae}\")\n",
    "    mae_list.append((f\"MAE using {neighbours_size} strongest positively correlated neighbours\", mae))\n",
    "\n",
    "    print(f\"RMSE using {neighbours_size} strongest positively correlated neighbours {rmse}\")\n",
    "    rmse_list.append((f\"RMSE using {neighbours_size} strongest positively coRrelated neighbours\", rmse))\n",
    "\n",
    "    p_error = evaluate_recommendation_quality_at_p(\n",
    "        p=10,\n",
    "        transformed_df=transformed_df,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores,  # Function to predict scores\n",
    "        userId=610,\n",
    "        k = neighbours_size\n",
    "    )\n",
    "    print(f\"Ma at p=10 using {neighbours_size} strongest positively coorelated neighbours {p_error}\")\n",
    "    perror_list.append((f\"p_error using {neighbours_size} strongest positively coorelated neighbours\", p_error))\n",
    "\n",
    "    overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=0,\n",
    "        weight_over=1,\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        predict_function=predict_scores,  # Function to predict scores\n",
    "        k = neighbours_size\n",
    "    )\n",
    "    print(f\"overestimate_error using {neighbours_size} strongest positively correlated neighbours {overestimate_error} \")\n",
    "    overestimate_error_list.append((f\"overestimate_error using {neighbours_size} strongest positively correlated neighbours\", overestimate_error))\n",
    "    underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "        transformed_df=transformed_df,\n",
    "        weight_under=1,\n",
    "        weight_over=0,\n",
    "        predict_function=predict_scores,  # Function to predict scores\n",
    "        userId=610,\n",
    "        num_splits=10,\n",
    "        k = neighbours_size\n",
    "    )\n",
    "    print(f\"underestimate_error using {neighbours_size} strongest positively correlated neighbours {underestimate_error}\")\n",
    "    underestimate_error_list.append((f\"underestimate_error using {neighbours_size} strongest positively correlated neighbours\", underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDTSDfLYRPFs"
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "This approach has yielded more promising results compared to the previous inclusion of negative correlations. However, the improvements are not significant. The RMSE did not improve compared to taking all positively correlated users, but both MAE and MAE at p=10 have improved. The choice of neighbourhood size (\\( k \\)) plays a crucial role in performance; it must be selected wisely. Overall, the best performance was achieved with 25 of the most similar users.\n",
    "\n",
    "Unfortunately, due to considering fewer data points when predicting ratings for each film, the overestimation and underestimation errors differ significantly. The algorithm tends to overestimate, which could be beneficial for a user with ample time for watching movies and who would not mind some false positive high ratings. In general, this is not the ideal outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64-jd6u1RPFs"
   },
   "source": [
    "### Change of approach\n",
    "\n",
    "Up until now, we have focused primarily on the correlation of our user with other users and their ratings for a particular film. While this approach seems reasonable, it does not account for one crucial aspect: the personal tendency of users to rate films either high or low. Some users may generally be grumpy or strict, while others might be more lenient overall in their grading. To address this, we need to consider how the rating for the film we are currently evaluating deviates from the given users’ rating means. This adjustment accounts for individual tendencies to rate films high or low, making the recommender system more versatile and accurate.\n",
    "\n",
    "We will continue to use the previously discovered threshold of the 25 strongest positively correlated users in this approach as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWOh8wo7RPFs",
    "outputId": "190a55de-c544-433d-bd0c-7aa052fc4cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 25 strongest positively correlated neighbours accounting for the personal  tendencies high/low 0.5400134448817265\n",
      "RMSE using 25 strongest positively correlated neighbours  accounting for the personal  tendencies high/low0.6820267320787797\n",
      "Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low 0.47314467607295896\n",
      "overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  0.5110370673816587 \n",
      "underestimate_error using  50 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  0.5572389916468498 \n"
     ]
    }
   ],
   "source": [
    "mae = evaluate_recommendation_quality_general(\n",
    "    transformed_df=transformed_df,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    k = 25\n",
    "      # Pass the threshold as a keyword argument\n",
    ")\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "    transformed_df=transformed_df,\n",
    "    predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"MAE using 25 strongest positively correlated neighbours accounting for the personal  tendencies high/low {mae}\")\n",
    "mae_list.append((f\"MAE using 25strongest positively correlated neighbours accounting for the personal tendencies high/low\",mae))\n",
    "\n",
    "print(f\"RMSE using 25 strongest positively correlated neighbours  accounting for the personal  tendencies high/low{rmse}\")\n",
    "rmse_list.append((f\"Rmse using 25strongest positively correlated neighbours accounting for the personal tendencies high/low\",rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "    p=10,\n",
    "    transformed_df=transformed_df,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"Ma at p=10 using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low {p_error}\")\n",
    "perror_list.append((f\"p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",p_error))\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=0,\n",
    "    weight_over=1,\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "    k = 25\n",
    ")\n",
    "print(f\"overestimate_error using  25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low  {overestimate_error} \")\n",
    "overestimate_error_list.append((f\"overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",overestimate_error))\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=1,\n",
    "    weight_over=0,\n",
    "    predict_function=predict_scores_no_baseline,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"underestimate_error using  {neighbours_size} strongest positively coorelated neighbours accounting for the personal  tendencies high/low  {underestimate_error} \")\n",
    "underestimate_error_list.append((f\"underestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low\",underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leW1Qj61RPFs"
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "This turned out to be a very effective approach. The MAE decreased visibly, and the RMSE improved even more significantly, dropping from the previous best of around 0.79 to 0.68 now. The overestimation and underestimation errors are very similar and significantly lower than before, which is also a very positive outcome. The error for the top 10 ratings decreased slightly, but only marginally compared to its previous best value. Overall, this approach seems to perform the best so far, as it minimizes error across all metrics, making it not only a good but also a versatile method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD-59gdFRPFs"
   },
   "source": [
    "### Item analysis\n",
    "\n",
    "Finally, why not analyze the items as well to improve our algorithm? The idea here is similar to the previous algorithm but with an added twist. After calculating the weighted deviation for a given (user ID, movie) tuple, we check if the score is equivocal. To clarify, consider an example: if the average weighted score suggests 3.2, it would be rounded down to 3 because this is the closest viable rating. However, it may also not be far from 3.5. In such a case, we check the mean value for this item (movie) to determine if it is above 3.5. If so, we set our average to 3.5. This approach allows for rounding up in scenarios where user behavior analysis is torn between two ratings.\n",
    "\n",
    "This addition of item analysis to the algorithm appears to be a great idea because it addresses ambiguity in user behavior where the scores are close to the rating boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qm6LnnfVRPFt",
    "outputId": "355edca9-1fd6-460b-f0e6-719601bbd610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity 0.5357688482689216\n",
      "RMSE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low0.7071960837101245\n",
      "Ma at p=10 using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity 0.48492063492063486\n",
      "overestimate_error using  25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity  0.7553065406303409 \n",
      "underestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity 0.4248038154063024 \n"
     ]
    }
   ],
   "source": [
    "mae = evaluate_recommendation_quality_general(\n",
    "    transformed_df=transformed_df,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "    userId=610,\n",
    "    k = 25\n",
    "      # Pass the threshold as a keyword argument\n",
    ")\n",
    "rmse = evaluate_recommendation_quality_rmse_general(\n",
    "    transformed_df=transformed_df,\n",
    "    predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"MAE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity {mae}\")\n",
    "mae_list.append((f\"MAE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity\",mae))\n",
    "\n",
    "print(f\"RMSE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low{rmse}\")\n",
    "rmse_list.append((f\"Rmse using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity\",rmse))\n",
    "\n",
    "p_error = evaluate_recommendation_quality_at_p(\n",
    "    p=10,\n",
    "    transformed_df=transformed_df,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "    userId=610,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"Ma at p=10 using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity {p_error}\")\n",
    "perror_list.append((f\"p_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity\",p_error))\n",
    "\n",
    "overestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=0,\n",
    "    weight_over=1,\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "    k = 25\n",
    ")\n",
    "print(f\"overestimate_error using  25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity  {overestimate_error} \")\n",
    "overestimate_error_list.append((f\"overestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity\",overestimate_error))\n",
    "underestimate_error = evaluate_recommendation_quality_general_weighted(\n",
    "    transformed_df=transformed_df,\n",
    "    weight_under=1,\n",
    "    weight_over=0,\n",
    "    predict_function=predict_scores_with_item_correction,  # Function to predict scores\n",
    "    userId=610,\n",
    "    num_splits=10,\n",
    "    k = 25\n",
    ")\n",
    "print(f\"underestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity {underestimate_error} \")\n",
    "underestimate_error_list.append((f\"underestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity\",underestimate_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaMmEZBsRPFt"
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "The difference compared to the previous method is not very striking. The MAE is slightly improved, but on the other hand, the RMSE is slightly worse, and the MAE at the 10 best ratings is also slightly lower. The underestimation error is lower than ever before, indicating that this system tends to overestimate, which could be the best choice for a user who is not very picky but wants to have a broad set of well-rated films to watch. In general, this idea seems quite interesting but likely requires more testing to make a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A69CdbwURPFt"
   },
   "source": [
    "# Results comparison and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv2uOqURRPFt",
    "outputId": "ea08a0dc-2217-4147-9b4e-d47c35d005c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE  scores:\n",
      "[('MAE using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity', 0.5357688482689216), ('MAE using 25strongest positively correlated neighbours accounting for the personal tendencies high/low', 0.5400134448817265), ('MAE using 25 strongest positively correlated neighbours', 0.5981621909345929), ('MAE using 50 strongest positively correlated neighbours', 0.6003440458652951), ('MAE using 0% positive correlation threshold', 0.6021952781071522), ('MAE using 15 strongest positively correlated neighbours', 0.6031413821118339), ('MAE using 0% positive and 0.6% negative correlation threshold', 0.604099744608393), ('MAE using 0% positive and 0.4% negative correlation threshold', 0.6060534511840754), ('MAE using 0.2% positive correlation threshold', 0.606241746389952), ('MAE using 0% positive and 0.6% negative correlation threshold', 0.6113749808614168), ('MAE using 0% positive and 0.2% negative correlation threshold', 0.6113749808614168), ('MAE using 5 strongest positively correlated neighbours', 0.6189454443760426), ('MAE using 0.4% positive correlation threshold', 0.6355510566787939), ('predict_baseline', 0.6701644157369347), ('MAE using 0.6% positive correlation threshold', 0.737954359048584)]\n",
      "\n",
      "\n",
      "rmse  scores:\n",
      "[('Rmse using 25strongest positively correlated neighbours accounting for the personal tendencies high/low', 0.6820267320787797), ('Rmse using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity', 0.7071960837101245), ('RMSE using 25 strongest positively coRrelated neighbours', 0.7906557039685967), ('RMSE using 50 strongest positively coRrelated neighbours', 0.7924685807405953), ('RMSE using 0% positive correlation threshold', 0.7939195306055609), ('RMSE using 15 strongest positively coRrelated neighbours', 0.7939928210640812), ('Rmse using 0.6 positive coorelation threshold', 0.7953086282698761), ('Rmse using 0.4 positive coorelation threshold', 0.7972685194076556), ('Rmse using 0.6 positive coorelation threshold', 0.80136603031125), ('Rmse using 0.2 positive coorelation threshold', 0.80136603031125), ('RMSE using 0.2% positive correlation threshold', 0.802867162923574), ('RMSE using 5 strongest positively coRrelated neighbours', 0.8134962988471877), ('RMSE using 0.4% positive correlation threshold', 0.8408924823724673), ('predict_baseline', 0.8670309338053681), ('RMSE using 0.6% positive correlation threshold', 0.933130514623205)]\n",
      "\n",
      "\n",
      "p_error at p =10   scores:\n",
      "[('p_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.47314467607295896), ('p_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity', 0.48492063492063486), ('p_error using 25 strongest positively coorelated neighbours', 0.485952380952381), ('p_error using 15 strongest positively coorelated neighbours', 0.4970634920634921), ('p_error using 50 strongest positively coorelated neighbours', 0.500952380952381), ('MAE at p=10 using 0 positive correlation threshold', 0.505952380952381), ('p_error using 0% positive and  0.6% negative correlation threshold', 0.505952380952381), ('p_error using 0% positive and  0.6% negative correlation threshold', 0.510952380952381), ('p_error using 0% positive and  0.2% negative correlation threshold', 0.510952380952381), ('p_error using 0% positive and  0.4% negative correlation threshold', 0.510952380952381), ('MAE at p=10 using 0.2 positive correlation threshold', 0.5147222222222223), ('MAE at p=10 using 0.4 positive correlation threshold', 0.5407936507936507), ('p_error using 5 strongest positively coorelated neighbours', 0.543015873015873), ('predict_baseline', 0.675), ('MAE at p=10 using 0.6 positive correlation threshold', nan)]\n",
      "\n",
      "\n",
      "underestimate errror  scores:\n",
      "[('underestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity', 0.4248038154063024), ('underestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.5572389916468498), ('underestimate_error using 25 strongest positively correlated neighbours', 0.57085036647656), ('underestimate_error using 50 strongest positively correlated neighbours', 0.5726464490431844), ('p_error using 0% positive correlation threshold', 0.574598358638441), ('underestimate_error using 0% positive and  0.6%  negative  coorelation threshold', 0.5766572684607717), ('underestimate_error using 15 strongest positively correlated neighbours', 0.5787871031982827), ('underestimate_error using 0% positive and  0.4%  negative  coorelation threshold', 0.5793087606016811), ('underestimate_error using 0% positive and  0.2%  negative  coorelation threshold', 0.5859043019335467), ('p_error using 0.2% positive correlation threshold', 0.5873767604119139), ('predict_baseline', 0.5890073252784332), ('underestimate_error using 5 strongest positively correlated neighbours', 0.5964032868467976), ('p_error using 0.4% positive correlation threshold', 0.6224698281566787), ('p_error using 0.6% positive correlation threshold', 0.6739252769765453)]\n",
      "\n",
      "\n",
      "overestimate error scores:\n",
      "[('overestimate_error using 25 strongest positively coorelated neighbours accounting for the personal  tendencies high/low', 0.5110370673816587), ('Overestimate MAE using 0.2% positive correlation threshold', 0.619356961510738), ('Overestimate MAE using 0% positive correlation threshold', 0.6211578619909098), ('Overestimate MAE using 0.4% positive correlation threshold', 0.6449831749158503), ('overestimate_error using 5 strongest positively correlated neighbours', 0.7107882585333297), ('overestimate_error using 15 strongest positively correlated neighbours', 0.7272806141227194), ('overestimate_erro using 0% positive and 0.4% negative correlation threshold', 0.7347326854718496), ('overestimate_error using 25 strongest positively correlated neighbours', 0.7353569459911625), ('overestimate_erro using 0% positive and 0.6% negative correlation threshold', 0.7360172704520868), ('overestimate_error using 50 strongest positively correlated neighbours', 0.7383173833826344), ('overestimate_erro using 0% positive and 0.2% negative correlation threshold', 0.745418158624428), ('overestimate_error using 25 strongest positively correlated neighbours accounting for the personal tendencies high/low and items specificity', 0.7553065406303409), ('Overestimate MAE using 0.6% positive correlation threshold', 0.7608267474862302), ('predict_baseline', 0.8334233572346182)]\n"
     ]
    }
   ],
   "source": [
    "mae_list.sort(key = lambda x: x[-1])\n",
    "rmse_list.sort(key = lambda x: x[-1])\n",
    "underestimate_error_list.sort(key = lambda x: x[-1])\n",
    "overestimate_error_list.sort(key = lambda x: x[-1])\n",
    "perror_list.sort(key = lambda x: x[-1])\n",
    "\n",
    "print (\"MAE  scores:\")\n",
    "print(mae_list)\n",
    "print('\\n')\n",
    "print (\"rmse  scores:\")\n",
    "print(rmse_list)\n",
    "print('\\n')\n",
    "print (\"p_error at p =10   scores:\")\n",
    "print(perror_list)\n",
    "print('\\n')\n",
    "print (\"underestimate errror  scores:\")\n",
    "print(underestimate_error_list)\n",
    "print('\\n')\n",
    "print (\"overestimate error scores:\")\n",
    "print(overestimate_error_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t3et0RgRPFt"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcJpOf-8RPFu"
   },
   "source": [
    "- We believe that we thoroughly tested several recommender system approaches, often building one upon another. The main conclusion is that the system based on the weighted average of deviations from the means for the 25 most correlated users performs the best. It is definitely the most versatile, performs well across all metrics, and is the top performer in some specific metrics, such as RMSE, which is an important measure for this problem.\n",
    "\n",
    "- Additionally, item analysis is a very promising idea. Even a simple conflict resolution approach using the mean for the given movie, applied when the weighted average of users yields an equivocal score, proved to be promising. With further work and testing, better results could likely be achieved using an item-based recommender system combined with a user-based one. We also tried combining a user-based recommender system based on correlation with an item-based recommender based on cosine similarity between movies. However, it turned out to be even more computationally expensive than the user-based recommender systems, leading us to abandon that approach.\n",
    "\n",
    "- The baseline approach, a very simple and inexpensive system, does not perform badly in terms of MAE. However, for other, potentially more informative measures, it is clearly inferior. It remains a useful reference or comparison point, as it is not significantly worse than other, more sophisticated approaches by an ‘order of magnitude.’\n",
    "\n",
    "- Our attempt to utilize negatively correlated users did not succeed. Although the idea appeared reasonable, introducing this concept only worsened performance across all metrics. It may be that the logic behind the algorithm was invalid, rather than the concept of using negative correlations per se."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
